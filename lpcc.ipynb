{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mfcc = 64\n",
    "n_mfcc_width = 432\n",
    "window_size = 10\n",
    "audio_len = 150\n",
    "data_dir = 'audio-train-new'\n",
    "n_samples = 118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa as lp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "from scikits.talkbox import lpc\n",
    "\n",
    "\n",
    "def convert_to_lpc(filename,number_of_coefficients):\n",
    "    wave, sr = lp.load(filename, mono=True, sr=None)\n",
    "    lpc_signal=lpc(wave,number_of_coefficients)\n",
    "#     print lpc_signal[0].shape, lpc_signal[1].shape, lpc_signal[2].shape\n",
    "#     print lpc_signal\n",
    "#     lpcc_signal=lpcc(lpc_signal[0],lpc_signal[1])\n",
    "#     print lpc_signal.shape\n",
    "    return np.hstack((lpc_signal[0],lpc_signal[1],lpc_signal[2]))\n",
    "                    \n",
    "\n",
    "def lpcc(seq, err_term, order=None):\n",
    "    if order is None:\n",
    "        order = len(seq) - 1\n",
    "    lpcc_coeffs = [np.log(err_term), -seq[0]]\n",
    "    for n in xrange(2, order + 1):\n",
    "        # Use order + 1 as upper bound for the last iteration\n",
    "        upbound = (order + 1 if n > order else n)\n",
    "        lpcc_coef = -sum(i * lpcc_coeffs[i] * seq[n - i - 1]\n",
    "                         for i in xrange(1, upbound)) * 1. / upbound\n",
    "        lpcc_coef -= seq[n - 1] if n <= len(seq) else 0\n",
    "        lpcc_coeffs.append(lpcc_coef)\n",
    "    return lpcc_coeffs\n",
    "\n",
    "\n",
    "def run_preprocess(root, length, split):\n",
    "    for subdir, dirs, files in os.walk(root):\n",
    "        for directory in dirs:\n",
    "            print(directory)\n",
    "            lpcc_data = []\n",
    "            npy_file = directory + '_' + 'lpcc' + '_'+ '.npy'\n",
    "#             if os.path.isfile(os.path.join(subdir, directory, npy_file)):\n",
    "#                 continue\n",
    "\n",
    "            if not os.path.isdir(os.path.join(subdir, directory, \"split\", split)):\n",
    "                subprocess.call([\"./preprocess\", os.path.join(subdir, directory), length, split])\n",
    "\n",
    "            file_path = os.path.join(subdir, directory, \"split\", split, \"wav\")\n",
    "            for filename in os.listdir(file_path):\n",
    "                lpcc_data.append(convert_to_lpc(os.path.join(file_path, filename),49))\n",
    "\n",
    "            if np.asarray(lpcc_data).shape[0] == 0:\n",
    "                continue\n",
    "            np.save(os.path.join(subdir, directory, npy_file), np.asarray(lpcc_data))\n",
    "            shutil.rmtree(os.path.join(subdir, directory, \"split\"), ignore_errors = True)\n",
    "#             print(np.asarray(lpcc_data).shape)\n",
    "        break\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def rename_npy(root, length, split):\n",
    "    for subdir, dirs, files in os.walk(root):\n",
    "        for directory in dirs:\n",
    "            npy_file = directory + '_' + length + '_' + split + '.npy'\n",
    "            new_npy_file = directory + '_' + 'mfcc' + '_' + str(512) + '_' + length + '_' + split + '.npy'\n",
    "            if os.path.isfile(os.path.join(subdir, directory, npy_file)):\n",
    "                shutil.move(os.path.join(subdir, directory, npy_file), os.path.join(subdir, directory, new_npy_file))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features(root):\n",
    "    lpcc_data=[]\n",
    "    lpcc_label = []\n",
    "    for subdir, dirs, files in os.walk(root):\n",
    "        count=0\n",
    "        for directory in dirs:\n",
    "            npy_file = directory + '_' + 'lpcc' + '_' + '.npy'\n",
    "            if(count==0):\n",
    "                lpcc_data= np.load(os.path.join(subdir, directory, npy_file))\n",
    "                print lpcc_data.shape\n",
    "                lpcc_label=lpcc_data.shape[0]*[directory.split('.')[0]]\n",
    "            else:\n",
    "                lpcc=np.load(os.path.join(subdir, directory, npy_file))\n",
    "                lpcc_data=np.vstack((lpcc_data,lpcc))\n",
    "                lpcc_label += lpcc.shape[0] * [directory.split('.')[0]]\n",
    "            count+=1\n",
    "        break\n",
    "    return lpcc_data, lpcc_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_split(root):\n",
    "    for subdir, dirs, files in os.walk(root):\n",
    "        for directory in dirs:\n",
    "            shutil.rmtree(os.path.join(subdir, directory, \"split\"), ignore_errors = True)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def cleanup_merged(root):\n",
    "    for subdir, dirs, files in os.walk(root):\n",
    "        for directory in dirs:\n",
    "            for f in glob.glob(os.path.join(subdir, directory, \"*_merged*.*\")):\n",
    "                os.remove(f)\n",
    "        break\n",
    "\n",
    "def cleanup_npy(root):\n",
    "    for subdir, dirs, files in os.walk(root):\n",
    "        for directory in dirs:\n",
    "            for f in glob.glob(os.path.join(subdir, directory, \"*.npy\")):\n",
    "                os.remove(f)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_samples(root):\n",
    "    for subdir, dirs, files in os.walk(root):\n",
    "        for directory in dirs:\n",
    "            shutil.move(os.path.join(subdir, directory), os.path.join(subdir, directory.split(\"_\")[0]))\n",
    "        break\n",
    "\n",
    "def remove_extra_samples(root):\n",
    "    for subdir, dirs, files in os.walk(root):\n",
    "        for directory in dirs:\n",
    "            name_o = directory.split(\".\") \n",
    "            if len(name_o) == 2 and int(name_o[1]) > 10:\n",
    "                shutil.rmtree(os.path.join(subdir, directory))\n",
    "        break\n",
    "    \n",
    "def distribute_samples(root):\n",
    "    for subdir, dirs, files in os.walk(root):\n",
    "        for directory in dirs:\n",
    "            if len(directory.split(\".\")) != 1:\n",
    "                continue\n",
    "            for d_subdir, d_dirs, d_files in os.walk(os.path.join(root, directory)):\n",
    "                for i, sample in enumerate(d_files):\n",
    "                    os.makedirs(os.path.join(subdir, directory + \".\" + str(i + 1)))\n",
    "                    shutil.move(os.path.join(subdir, directory, sample), \\\n",
    "                                os.path.join(subdir, directory + \".\" + str(i + 1), sample))\n",
    "                break\n",
    "        remove_extra_samples(root)\n",
    "        break\n",
    "        \n",
    "def combine_samples(root):\n",
    "    for subdir, dirs, files in os.walk(root):\n",
    "        for directory in dirs:\n",
    "            if not os.path.isdir(os.path.join(subdir, directory.split(\".\")[0])):\n",
    "                os.makedirs(os.path.join(subdir, directory.split(\".\")[0]))\n",
    "            else:\n",
    "                for f in glob.glob(os.path.join(subdir, directory.split(\".\")[0], \"*.npy\")):\n",
    "                    os.remove(f)\n",
    "                    shutil.rmtree(os.path.join(subdir, directory.split(\".\")[0], \"split\"), ignore_errors = True)\n",
    "            for d_subdir, d_dirs, d_files in os.walk(os.path.join(root, directory)):\n",
    "                mp3_files = glob.glob(os.path.join(subdir, directory, \"*.mp3\"))\n",
    "                for i, sample in enumerate(mp3_files):\n",
    "                    shutil.move(os.path.join(subdir, directory, os.path.basename(sample)), \\\n",
    "                                os.path.join(subdir, directory.split(\".\")[0], os.path.basename(sample)))\n",
    "                if len(directory.split(\".\")) != 1:\n",
    "                    shutil.rmtree(os.path.join(subdir, directory))\n",
    "                break\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adventuresbobwhite\n",
      "aequanimitas\n",
      "agnesgreyversion3\n",
      "airplaneflyinghandbookvol3\n",
      "alondonlife\n",
      "ancient\n",
      "aprendizdeconspirador\n",
      "aristotles\n",
      "artofdivinecontentment\n",
      "battle-pieces\n",
      "boatsoftheglencarrig\n",
      "bookofgoodcounsels\n",
      "briefe\n",
      "britishsubject\n",
      "canti\n",
      "childs\n",
      "company\n",
      "contes\n",
      "country\n",
      "curlyandfloppytwistytail\n",
      "de\n",
      "deepwoodstocivilization\n",
      "dernier\n",
      "door\n",
      "dubrovsky\n",
      "englishgirls\n",
      "esau\n",
      "essayoncriticism\n",
      "experiences\n",
      "explorersandtravellers\n",
      "food\n",
      "formation\n",
      "franceatwar\n",
      "greybeardsatplay\n",
      "gullivers\n",
      "hindu\n",
      "his\n",
      "historietas\n",
      "historymathematics\n",
      "how\n",
      "human\n",
      "jessejames\n",
      "john\n",
      "journal\n",
      "konekgorbunok\n",
      "lark\n",
      "lebenssucher\n",
      "letters\n",
      "lifeofcarltonparker\n",
      "little\n",
      "littlebrothertothebear\n",
      "littlefoldedhands\n",
      "littleprincess\n",
      "lob\n",
      "marcia\n",
      "marybartonversion2\n",
      "mayorofcasterbridge3\n",
      "meguriai\n",
      "meister\n",
      "miss\n",
      "modern\n",
      "moral\n",
      "morallettersvol1\n",
      "mounties\n",
      "myreminiscences\n",
      "mysteries\n",
      "mystical\n",
      "myths\n",
      "nedfranks\n",
      "new\n",
      "onanirishjauntingcar\n",
      "origin\n",
      "pennycomequicks\n",
      "philosophical\n",
      "plateroyyo\n",
      "psychologyofpeople\n",
      "religiousaffections\n",
      "republicofthefuture\n",
      "reynardthefox\n",
      "rose\n",
      "roughriders\n",
      "sabotage\n",
      "sacred\n",
      "san\n",
      "sexesinscienceandhistory\n",
      "shininggateway\n",
      "short\n",
      "stine\n",
      "story\n",
      "symbolism\n",
      "tausend\n",
      "tenfrominfinity\n",
      "the\n",
      "theodoric\n",
      "thirdperson\n",
      "threetimesandout\n",
      "tour\n",
      "tower\n",
      "tschun\n",
      "twice\n",
      "uncle\n",
      "uncletomscabin\n",
      "unknown\n",
      "visha\n",
      "warden\n",
      "weisse\n",
      "wildknight\n",
      "wildwales\n",
      "winning\n",
      "worshipper\n",
      "young\n",
      "youngwomansguide\n"
     ]
    }
   ],
   "source": [
    "cleanup_split(data_dir)\n",
    "# cleanup_npy(data_dir)\n",
    "# rename_samples(data_dir)\n",
    "# distribute_samples(data_dir)\n",
    "# combine_samples(data_dir)\n",
    "run_preprocess(data_dir, str(audio_len), str(window_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 100)\n",
      "(1680, 100)\n"
     ]
    }
   ],
   "source": [
    "# from IPython.core.debugger import set_trace\n",
    "X, y = load_features(data_dir)\n",
    "print X.shape\n",
    "# X=np.asarray(X)\n",
    "# y=np.asarray(y)\n",
    "X = X.reshape(X.shape[0], 10, -1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1680, 10, 10, 1)\n"
     ]
    }
   ],
   "source": [
    "print X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1680, 10, 10, 1)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-167-ed279da4f092>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "\n",
    "l_enc = LabelEncoder()\n",
    "l_enc.fit(y_train)\n",
    "y_train_enc = l_enc.transform(y_train)\n",
    "y_train_norm = np_utils.to_categorical(y_train_enc)\n",
    "\n",
    "l_enc.fit(y_test)\n",
    "y_test_enc = l_enc.transform(y_test)\n",
    "y_test_norm = np_utils.to_categorical(y_test_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del X, y, y_train_enc, y_test_enc, y_train, y_test\n",
    "# X_train = X_train\n",
    "# X_test = X_test\n",
    "# X_r = X_test.reshape(X_test.shape[0], 5, -1, 1)\n",
    "# X_r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(1)\n",
    "from keras.layers import Activation, Conv2D, Dense, Dropout, Flatten, MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(2, 2), activation='relu',\n",
    "                 input_shape=(10, 10, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(2, 2), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dense(64))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(len(y_test_norm[0])))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "ctr = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='mfcc_model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_18 (Conv2D)           (None, 24, 0, 32)         160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 12, 0, 32)         0         \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 12, 0, 32)         0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 12, 0, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 12, 0, 32)         4128      \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 12, 0, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 6, 0, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 6, 0, 32)          0         \n",
      "=================================================================\n",
      "Total params: 4,288\n",
      "Trainable params: 4,288\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1176 samples, validate on 504 samples\n",
      "Epoch 1/500\n",
      "1176/1176 [==============================] - 1s 716us/step - loss: 4.7515 - acc: 0.0060 - val_loss: 4.7586 - val_acc: 0.0139\n",
      "Epoch 2/500\n",
      "1176/1176 [==============================] - 1s 488us/step - loss: 4.7105 - acc: 0.0119 - val_loss: 4.6773 - val_acc: 0.0119\n",
      "Epoch 3/500\n",
      "1176/1176 [==============================] - 1s 467us/step - loss: 4.6801 - acc: 0.0145 - val_loss: 4.6429 - val_acc: 0.0198\n",
      "Epoch 4/500\n",
      "1176/1176 [==============================] - 1s 465us/step - loss: 4.6490 - acc: 0.0221 - val_loss: 4.6231 - val_acc: 0.0218\n",
      "Epoch 5/500\n",
      "1176/1176 [==============================] - 1s 569us/step - loss: 4.6297 - acc: 0.0170 - val_loss: 4.6153 - val_acc: 0.0198\n",
      "Epoch 6/500\n",
      "1176/1176 [==============================] - 1s 558us/step - loss: 4.6088 - acc: 0.0187 - val_loss: 4.5928 - val_acc: 0.0159\n",
      "Epoch 7/500\n",
      "1176/1176 [==============================] - 1s 497us/step - loss: 4.5919 - acc: 0.0221 - val_loss: 4.5890 - val_acc: 0.0179\n",
      "Epoch 8/500\n",
      "1176/1176 [==============================] - 1s 548us/step - loss: 4.5756 - acc: 0.0187 - val_loss: 4.5807 - val_acc: 0.0159\n",
      "Epoch 9/500\n",
      "1176/1176 [==============================] - 1s 521us/step - loss: 4.5590 - acc: 0.0187 - val_loss: 4.5606 - val_acc: 0.0159\n",
      "Epoch 10/500\n",
      "1176/1176 [==============================] - 1s 533us/step - loss: 4.5460 - acc: 0.0247 - val_loss: 4.5570 - val_acc: 0.0198\n",
      "Epoch 11/500\n",
      "1176/1176 [==============================] - 1s 589us/step - loss: 4.5368 - acc: 0.0213 - val_loss: 4.5464 - val_acc: 0.0238\n",
      "Epoch 12/500\n",
      "1176/1176 [==============================] - 1s 579us/step - loss: 4.5171 - acc: 0.0289 - val_loss: 4.5345 - val_acc: 0.0278\n",
      "Epoch 13/500\n",
      "1176/1176 [==============================] - 1s 510us/step - loss: 4.5005 - acc: 0.0400 - val_loss: 4.5190 - val_acc: 0.0337\n",
      "Epoch 14/500\n",
      "1176/1176 [==============================] - 1s 508us/step - loss: 4.4901 - acc: 0.0383 - val_loss: 4.5049 - val_acc: 0.0317\n",
      "Epoch 15/500\n",
      "1176/1176 [==============================] - 1s 519us/step - loss: 4.4654 - acc: 0.0485 - val_loss: 4.4935 - val_acc: 0.0476\n",
      "Epoch 16/500\n",
      "1176/1176 [==============================] - 1s 553us/step - loss: 4.4613 - acc: 0.0459 - val_loss: 4.4806 - val_acc: 0.0516\n",
      "Epoch 17/500\n",
      "1176/1176 [==============================] - 1s 542us/step - loss: 4.4174 - acc: 0.0595 - val_loss: 4.4548 - val_acc: 0.0516\n",
      "Epoch 18/500\n",
      "1176/1176 [==============================] - 1s 598us/step - loss: 4.4102 - acc: 0.0604 - val_loss: 4.4675 - val_acc: 0.0615\n",
      "Epoch 19/500\n",
      "1176/1176 [==============================] - 1s 487us/step - loss: 4.3696 - acc: 0.0612 - val_loss: 4.4383 - val_acc: 0.0595\n",
      "Epoch 20/500\n",
      "1176/1176 [==============================] - 1s 569us/step - loss: 4.3717 - acc: 0.0680 - val_loss: 4.4261 - val_acc: 0.0635\n",
      "Epoch 21/500\n",
      "1176/1176 [==============================] - 1s 635us/step - loss: 4.3336 - acc: 0.0757 - val_loss: 4.4206 - val_acc: 0.0635\n",
      "Epoch 22/500\n",
      "1176/1176 [==============================] - 1s 567us/step - loss: 4.2931 - acc: 0.0757 - val_loss: 4.4004 - val_acc: 0.0754\n",
      "Epoch 23/500\n",
      "1176/1176 [==============================] - 1s 637us/step - loss: 4.2654 - acc: 0.0850 - val_loss: 4.3805 - val_acc: 0.0734\n",
      "Epoch 24/500\n",
      "1176/1176 [==============================] - 1s 501us/step - loss: 4.2299 - acc: 0.0918 - val_loss: 4.3557 - val_acc: 0.0794\n",
      "Epoch 25/500\n",
      "1176/1176 [==============================] - 1s 480us/step - loss: 4.2054 - acc: 0.0910 - val_loss: 4.3278 - val_acc: 0.0813\n",
      "Epoch 26/500\n",
      "1176/1176 [==============================] - 1s 497us/step - loss: 4.1966 - acc: 0.0884 - val_loss: 4.3361 - val_acc: 0.0774\n",
      "Epoch 27/500\n",
      "1176/1176 [==============================] - 1s 573us/step - loss: 4.1744 - acc: 0.0876 - val_loss: 4.3152 - val_acc: 0.0694\n",
      "Epoch 28/500\n",
      "1176/1176 [==============================] - 1s 478us/step - loss: 4.1597 - acc: 0.0952 - val_loss: 4.2653 - val_acc: 0.0992\n",
      "Epoch 29/500\n",
      "1176/1176 [==============================] - 1s 472us/step - loss: 4.0858 - acc: 0.1182 - val_loss: 4.2332 - val_acc: 0.0933\n",
      "Epoch 30/500\n",
      "1176/1176 [==============================] - 1s 479us/step - loss: 4.0632 - acc: 0.1131 - val_loss: 4.1997 - val_acc: 0.1111\n",
      "Epoch 31/500\n",
      "1176/1176 [==============================] - 1s 479us/step - loss: 4.0328 - acc: 0.1080 - val_loss: 4.1852 - val_acc: 0.1230\n",
      "Epoch 32/500\n",
      "1176/1176 [==============================] - 1s 469us/step - loss: 4.0009 - acc: 0.1267 - val_loss: 4.1443 - val_acc: 0.1290\n",
      "Epoch 33/500\n",
      "1176/1176 [==============================] - 1s 475us/step - loss: 3.9867 - acc: 0.1395 - val_loss: 4.1401 - val_acc: 0.1448\n",
      "Epoch 34/500\n",
      "1176/1176 [==============================] - 1s 487us/step - loss: 3.9398 - acc: 0.1446 - val_loss: 4.0887 - val_acc: 0.1508\n",
      "Epoch 35/500\n",
      "1176/1176 [==============================] - 1s 476us/step - loss: 3.9130 - acc: 0.1386 - val_loss: 4.0731 - val_acc: 0.1607\n",
      "Epoch 36/500\n",
      "1176/1176 [==============================] - 1s 478us/step - loss: 3.8984 - acc: 0.1344 - val_loss: 4.0327 - val_acc: 0.1687\n",
      "Epoch 37/500\n",
      "1176/1176 [==============================] - 1s 479us/step - loss: 3.8458 - acc: 0.1531 - val_loss: 3.9917 - val_acc: 0.1746\n",
      "Epoch 38/500\n",
      "1176/1176 [==============================] - 1s 482us/step - loss: 3.8426 - acc: 0.1514 - val_loss: 3.9701 - val_acc: 0.1845\n",
      "Epoch 39/500\n",
      "1176/1176 [==============================] - 1s 469us/step - loss: 3.8105 - acc: 0.1599 - val_loss: 3.9463 - val_acc: 0.1806\n",
      "Epoch 40/500\n",
      "1176/1176 [==============================] - 1s 476us/step - loss: 3.7825 - acc: 0.1616 - val_loss: 3.9433 - val_acc: 0.1964\n",
      "Epoch 41/500\n",
      "1176/1176 [==============================] - 1s 473us/step - loss: 3.7497 - acc: 0.1786 - val_loss: 3.9081 - val_acc: 0.1925\n",
      "Epoch 42/500\n",
      "1176/1176 [==============================] - 1s 481us/step - loss: 3.6904 - acc: 0.1803 - val_loss: 3.8828 - val_acc: 0.2123\n",
      "Epoch 43/500\n",
      "1176/1176 [==============================] - 1s 480us/step - loss: 3.6796 - acc: 0.2075 - val_loss: 3.8472 - val_acc: 0.2222\n",
      "Epoch 44/500\n",
      "1176/1176 [==============================] - 1s 477us/step - loss: 3.6141 - acc: 0.2041 - val_loss: 3.7983 - val_acc: 0.2302\n",
      "Epoch 45/500\n",
      "1176/1176 [==============================] - 1s 469us/step - loss: 3.5748 - acc: 0.2134 - val_loss: 3.7812 - val_acc: 0.2282\n",
      "Epoch 46/500\n",
      "1176/1176 [==============================] - 1s 490us/step - loss: 3.6037 - acc: 0.2245 - val_loss: 3.7325 - val_acc: 0.2401\n",
      "Epoch 47/500\n",
      "1176/1176 [==============================] - 1s 480us/step - loss: 3.5632 - acc: 0.2296 - val_loss: 3.7145 - val_acc: 0.2440\n",
      "Epoch 48/500\n",
      "1176/1176 [==============================] - 1s 474us/step - loss: 3.4461 - acc: 0.2398 - val_loss: 3.6862 - val_acc: 0.2500\n",
      "Epoch 49/500\n",
      "1176/1176 [==============================] - 1s 648us/step - loss: 3.5112 - acc: 0.2381 - val_loss: 3.6505 - val_acc: 0.2837\n",
      "Epoch 50/500\n",
      "1176/1176 [==============================] - 1s 518us/step - loss: 3.4623 - acc: 0.2406 - val_loss: 3.5986 - val_acc: 0.2698\n",
      "Epoch 51/500\n",
      "1176/1176 [==============================] - 1s 549us/step - loss: 3.4582 - acc: 0.2381 - val_loss: 3.5947 - val_acc: 0.2877\n",
      "Epoch 52/500\n",
      "1176/1176 [==============================] - 1s 529us/step - loss: 3.4278 - acc: 0.2398 - val_loss: 3.5511 - val_acc: 0.2956\n",
      "Epoch 53/500\n",
      "1176/1176 [==============================] - 1s 488us/step - loss: 3.3344 - acc: 0.2483 - val_loss: 3.5272 - val_acc: 0.2976\n",
      "Epoch 54/500\n",
      "1176/1176 [==============================] - 1s 515us/step - loss: 3.3470 - acc: 0.2798 - val_loss: 3.5023 - val_acc: 0.3056\n",
      "Epoch 55/500\n",
      "1176/1176 [==============================] - 1s 482us/step - loss: 3.3147 - acc: 0.2653 - val_loss: 3.4728 - val_acc: 0.3254\n",
      "Epoch 56/500\n",
      "1176/1176 [==============================] - 1s 479us/step - loss: 3.2793 - acc: 0.2874 - val_loss: 3.4271 - val_acc: 0.3175\n",
      "Epoch 57/500\n",
      "1176/1176 [==============================] - 1s 528us/step - loss: 3.2767 - acc: 0.2832 - val_loss: 3.3994 - val_acc: 0.3313\n",
      "Epoch 58/500\n",
      "1176/1176 [==============================] - 1s 520us/step - loss: 3.2116 - acc: 0.2823 - val_loss: 3.3682 - val_acc: 0.3254\n",
      "Epoch 59/500\n",
      "1176/1176 [==============================] - 1s 537us/step - loss: 3.1709 - acc: 0.3146 - val_loss: 3.3307 - val_acc: 0.3452\n",
      "Epoch 60/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1176/1176 [==============================] - 1s 477us/step - loss: 3.2582 - acc: 0.2959 - val_loss: 3.3002 - val_acc: 0.3552\n",
      "Epoch 61/500\n",
      "1176/1176 [==============================] - 1s 548us/step - loss: 3.1113 - acc: 0.2976 - val_loss: 3.2740 - val_acc: 0.3552\n",
      "Epoch 62/500\n",
      "1176/1176 [==============================] - 1s 540us/step - loss: 3.0879 - acc: 0.3223 - val_loss: 3.2412 - val_acc: 0.3571\n",
      "Epoch 63/500\n",
      "1176/1176 [==============================] - 1s 482us/step - loss: 3.0998 - acc: 0.3061 - val_loss: 3.2156 - val_acc: 0.3730\n",
      "Epoch 64/500\n",
      "1176/1176 [==============================] - 1s 489us/step - loss: 3.0739 - acc: 0.3180 - val_loss: 3.1784 - val_acc: 0.3929\n",
      "Epoch 65/500\n",
      "1176/1176 [==============================] - 1s 485us/step - loss: 3.0604 - acc: 0.3189 - val_loss: 3.1443 - val_acc: 0.3968\n",
      "Epoch 66/500\n",
      "1176/1176 [==============================] - 1s 518us/step - loss: 3.0302 - acc: 0.3265 - val_loss: 3.1182 - val_acc: 0.3968\n",
      "Epoch 67/500\n",
      "1176/1176 [==============================] - 1s 507us/step - loss: 2.9764 - acc: 0.3418 - val_loss: 3.0828 - val_acc: 0.4067\n",
      "Epoch 68/500\n",
      "1176/1176 [==============================] - 1s 509us/step - loss: 2.9330 - acc: 0.3605 - val_loss: 3.0561 - val_acc: 0.4167\n",
      "Epoch 69/500\n",
      "1176/1176 [==============================] - 1s 472us/step - loss: 2.9522 - acc: 0.3452 - val_loss: 3.0286 - val_acc: 0.4107\n",
      "Epoch 70/500\n",
      "1176/1176 [==============================] - 1s 479us/step - loss: 2.9635 - acc: 0.3529 - val_loss: 3.0390 - val_acc: 0.4286\n",
      "Epoch 71/500\n",
      "1176/1176 [==============================] - 1s 476us/step - loss: 2.9290 - acc: 0.3690 - val_loss: 2.9861 - val_acc: 0.4365\n",
      "Epoch 72/500\n",
      "1176/1176 [==============================] - 1s 481us/step - loss: 2.8816 - acc: 0.3529 - val_loss: 2.9701 - val_acc: 0.4444\n",
      "Epoch 73/500\n",
      "1176/1176 [==============================] - 1s 474us/step - loss: 2.8567 - acc: 0.3639 - val_loss: 2.9317 - val_acc: 0.4504\n",
      "Epoch 74/500\n",
      "1176/1176 [==============================] - 1s 473us/step - loss: 2.8435 - acc: 0.3767 - val_loss: 2.8884 - val_acc: 0.4583\n",
      "Epoch 75/500\n",
      "1176/1176 [==============================] - 1s 473us/step - loss: 2.7852 - acc: 0.3707 - val_loss: 2.8570 - val_acc: 0.4583\n",
      "Epoch 76/500\n",
      "1176/1176 [==============================] - 1s 475us/step - loss: 2.7618 - acc: 0.3801 - val_loss: 2.8252 - val_acc: 0.4762\n",
      "Epoch 77/500\n",
      "1176/1176 [==============================] - 1s 475us/step - loss: 2.7078 - acc: 0.3895 - val_loss: 2.7957 - val_acc: 0.4802\n",
      "Epoch 78/500\n",
      "1176/1176 [==============================] - 1s 474us/step - loss: 2.7172 - acc: 0.3937 - val_loss: 2.7791 - val_acc: 0.4782\n",
      "Epoch 79/500\n",
      "1176/1176 [==============================] - 1s 474us/step - loss: 2.7178 - acc: 0.3827 - val_loss: 2.7522 - val_acc: 0.4782\n",
      "Epoch 80/500\n",
      "1176/1176 [==============================] - 1s 471us/step - loss: 2.6568 - acc: 0.4031 - val_loss: 2.7267 - val_acc: 0.4861\n",
      "Epoch 81/500\n",
      "1176/1176 [==============================] - 1s 469us/step - loss: 2.6750 - acc: 0.4014 - val_loss: 2.7153 - val_acc: 0.4861\n",
      "Epoch 82/500\n",
      "1176/1176 [==============================] - 1s 469us/step - loss: 2.6189 - acc: 0.4201 - val_loss: 2.6917 - val_acc: 0.5099\n",
      "Epoch 83/500\n",
      "1176/1176 [==============================] - 1s 471us/step - loss: 2.6758 - acc: 0.4039 - val_loss: 2.6839 - val_acc: 0.5119\n",
      "Epoch 84/500\n",
      "1176/1176 [==============================] - 1s 470us/step - loss: 2.6156 - acc: 0.4201 - val_loss: 2.6463 - val_acc: 0.5040\n",
      "Epoch 85/500\n",
      "1176/1176 [==============================] - 1s 471us/step - loss: 2.5548 - acc: 0.4141 - val_loss: 2.6193 - val_acc: 0.5139\n",
      "Epoch 86/500\n",
      "1176/1176 [==============================] - 1s 472us/step - loss: 2.5355 - acc: 0.4481 - val_loss: 2.5986 - val_acc: 0.5278\n",
      "Epoch 87/500\n",
      "1176/1176 [==============================] - 1s 516us/step - loss: 2.5105 - acc: 0.4235 - val_loss: 2.5678 - val_acc: 0.5317\n",
      "Epoch 88/500\n",
      "1176/1176 [==============================] - 1s 597us/step - loss: 2.5146 - acc: 0.4192 - val_loss: 2.5517 - val_acc: 0.5357\n",
      "Epoch 89/500\n",
      "1176/1176 [==============================] - 1s 523us/step - loss: 2.5331 - acc: 0.4422 - val_loss: 2.5323 - val_acc: 0.5496\n",
      "Epoch 90/500\n",
      "1176/1176 [==============================] - 1s 499us/step - loss: 2.5132 - acc: 0.4269 - val_loss: 2.5358 - val_acc: 0.5615\n",
      "Epoch 91/500\n",
      "1176/1176 [==============================] - 1s 497us/step - loss: 2.4882 - acc: 0.4558 - val_loss: 2.5401 - val_acc: 0.5575\n",
      "Epoch 92/500\n",
      "1176/1176 [==============================] - 1s 528us/step - loss: 2.4467 - acc: 0.4405 - val_loss: 2.5056 - val_acc: 0.5655\n",
      "Epoch 93/500\n",
      "1176/1176 [==============================] - 1s 483us/step - loss: 2.4840 - acc: 0.4422 - val_loss: 2.4652 - val_acc: 0.5675\n",
      "Epoch 94/500\n",
      "1176/1176 [==============================] - 1s 560us/step - loss: 2.4682 - acc: 0.4549 - val_loss: 2.4363 - val_acc: 0.5675\n",
      "Epoch 95/500\n",
      "1176/1176 [==============================] - 1s 485us/step - loss: 2.3921 - acc: 0.4762 - val_loss: 2.4115 - val_acc: 0.5794\n",
      "Epoch 96/500\n",
      "1176/1176 [==============================] - 1s 480us/step - loss: 2.4386 - acc: 0.4626 - val_loss: 2.3871 - val_acc: 0.5734\n",
      "Epoch 97/500\n",
      "1176/1176 [==============================] - 1s 478us/step - loss: 2.3825 - acc: 0.4762 - val_loss: 2.3786 - val_acc: 0.5933\n",
      "Epoch 98/500\n",
      "1176/1176 [==============================] - 1s 480us/step - loss: 2.3400 - acc: 0.4677 - val_loss: 2.3577 - val_acc: 0.5893\n",
      "Epoch 99/500\n",
      "1176/1176 [==============================] - 1s 473us/step - loss: 2.3062 - acc: 0.4668 - val_loss: 2.3417 - val_acc: 0.5893\n",
      "Epoch 100/500\n",
      "1176/1176 [==============================] - 1s 476us/step - loss: 2.2783 - acc: 0.4923 - val_loss: 2.3366 - val_acc: 0.5873\n",
      "Epoch 101/500\n",
      "1176/1176 [==============================] - 1s 476us/step - loss: 2.3360 - acc: 0.4804 - val_loss: 2.3050 - val_acc: 0.5893\n",
      "Epoch 102/500\n",
      "1176/1176 [==============================] - 1s 472us/step - loss: 2.3309 - acc: 0.4898 - val_loss: 2.2774 - val_acc: 0.5972\n",
      "Epoch 103/500\n",
      "1176/1176 [==============================] - 1s 476us/step - loss: 2.2860 - acc: 0.4855 - val_loss: 2.2605 - val_acc: 0.6012\n",
      "Epoch 104/500\n",
      "1176/1176 [==============================] - 1s 487us/step - loss: 2.2740 - acc: 0.4940 - val_loss: 2.2424 - val_acc: 0.6052\n",
      "Epoch 105/500\n",
      "1176/1176 [==============================] - 1s 476us/step - loss: 2.2782 - acc: 0.4957 - val_loss: 2.2328 - val_acc: 0.6032\n",
      "Epoch 106/500\n",
      "1176/1176 [==============================] - 1s 470us/step - loss: 2.1754 - acc: 0.4949 - val_loss: 2.2099 - val_acc: 0.6131\n",
      "Epoch 107/500\n",
      "1176/1176 [==============================] - 1s 475us/step - loss: 2.1818 - acc: 0.5136 - val_loss: 2.1921 - val_acc: 0.6310\n",
      "Epoch 108/500\n",
      "1176/1176 [==============================] - 1s 478us/step - loss: 2.1601 - acc: 0.5068 - val_loss: 2.1893 - val_acc: 0.6329\n",
      "Epoch 109/500\n",
      "1176/1176 [==============================] - 1s 485us/step - loss: 2.1186 - acc: 0.5094 - val_loss: 2.1710 - val_acc: 0.6329\n",
      "Epoch 110/500\n",
      "1176/1176 [==============================] - 1s 488us/step - loss: 2.1188 - acc: 0.5332 - val_loss: 2.1545 - val_acc: 0.6171\n",
      "Epoch 111/500\n",
      "1176/1176 [==============================] - 1s 497us/step - loss: 2.1160 - acc: 0.5408 - val_loss: 2.1224 - val_acc: 0.6290\n",
      "Epoch 112/500\n",
      "1176/1176 [==============================] - 1s 477us/step - loss: 2.1987 - acc: 0.5170 - val_loss: 2.1163 - val_acc: 0.6290\n",
      "Epoch 113/500\n",
      "1176/1176 [==============================] - 1s 474us/step - loss: 2.1329 - acc: 0.5264 - val_loss: 2.1060 - val_acc: 0.6369\n",
      "Epoch 114/500\n",
      "1176/1176 [==============================] - 1s 473us/step - loss: 2.0972 - acc: 0.5332 - val_loss: 2.0877 - val_acc: 0.6409\n",
      "Epoch 115/500\n",
      "1176/1176 [==============================] - 1s 477us/step - loss: 2.0484 - acc: 0.5459 - val_loss: 2.0776 - val_acc: 0.6488\n",
      "Epoch 116/500\n",
      "1176/1176 [==============================] - 1s 476us/step - loss: 2.0995 - acc: 0.5349 - val_loss: 2.0530 - val_acc: 0.6567\n",
      "Epoch 117/500\n",
      "1176/1176 [==============================] - 1s 476us/step - loss: 2.0591 - acc: 0.5272 - val_loss: 2.0380 - val_acc: 0.6548\n",
      "Epoch 118/500\n",
      "1176/1176 [==============================] - 1s 474us/step - loss: 2.0472 - acc: 0.5442 - val_loss: 2.0352 - val_acc: 0.6488\n",
      "Epoch 119/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1176/1176 [==============================] - 1s 474us/step - loss: 2.0926 - acc: 0.5468 - val_loss: 2.0190 - val_acc: 0.6508\n",
      "Epoch 120/500\n",
      "1176/1176 [==============================] - 1s 481us/step - loss: 2.0116 - acc: 0.5485 - val_loss: 2.0010 - val_acc: 0.6548\n",
      "Epoch 121/500\n",
      "1176/1176 [==============================] - 1s 477us/step - loss: 1.9597 - acc: 0.5612 - val_loss: 1.9828 - val_acc: 0.6647\n",
      "Epoch 122/500\n",
      "1176/1176 [==============================] - 1s 476us/step - loss: 1.9573 - acc: 0.5655 - val_loss: 1.9828 - val_acc: 0.6627\n",
      "Epoch 123/500\n",
      "1176/1176 [==============================] - 1s 493us/step - loss: 1.9633 - acc: 0.5697 - val_loss: 2.0168 - val_acc: 0.6567\n",
      "Epoch 124/500\n",
      "1176/1176 [==============================] - 1s 464us/step - loss: 1.9302 - acc: 0.5527 - val_loss: 2.0046 - val_acc: 0.6587\n",
      "Epoch 125/500\n",
      "1176/1176 [==============================] - 1s 457us/step - loss: 1.9218 - acc: 0.5621 - val_loss: 1.9640 - val_acc: 0.6647\n",
      "Epoch 126/500\n",
      "1176/1176 [==============================] - 1s 464us/step - loss: 2.0252 - acc: 0.5595 - val_loss: 1.9730 - val_acc: 0.6667\n",
      "Epoch 127/500\n",
      "1176/1176 [==============================] - 1s 467us/step - loss: 1.9869 - acc: 0.5765 - val_loss: 1.9371 - val_acc: 0.6667\n",
      "Epoch 128/500\n",
      "1176/1176 [==============================] - 1s 495us/step - loss: 1.9659 - acc: 0.5502 - val_loss: 1.9153 - val_acc: 0.6706\n",
      "Epoch 129/500\n",
      "1176/1176 [==============================] - 1s 482us/step - loss: 1.8469 - acc: 0.5791 - val_loss: 1.9158 - val_acc: 0.6726\n",
      "Epoch 130/500\n",
      "1176/1176 [==============================] - 1s 465us/step - loss: 1.9173 - acc: 0.5689 - val_loss: 1.8760 - val_acc: 0.6746\n",
      "Epoch 131/500\n",
      "1176/1176 [==============================] - 1s 476us/step - loss: 1.8460 - acc: 0.5884 - val_loss: 1.8852 - val_acc: 0.6786\n",
      "Epoch 132/500\n",
      "1176/1176 [==============================] - 1s 478us/step - loss: 1.8502 - acc: 0.6029 - val_loss: 1.8509 - val_acc: 0.6845\n",
      "Epoch 133/500\n",
      "1176/1176 [==============================] - 1s 474us/step - loss: 1.8558 - acc: 0.5825 - val_loss: 1.8595 - val_acc: 0.6845\n",
      "Epoch 134/500\n",
      "1176/1176 [==============================] - 1s 474us/step - loss: 1.8421 - acc: 0.5867 - val_loss: 1.8387 - val_acc: 0.6885\n",
      "Epoch 135/500\n",
      "1176/1176 [==============================] - 1s 472us/step - loss: 1.8394 - acc: 0.5825 - val_loss: 1.8335 - val_acc: 0.6845\n",
      "Epoch 136/500\n",
      "1176/1176 [==============================] - 1s 475us/step - loss: 1.8031 - acc: 0.5927 - val_loss: 1.8098 - val_acc: 0.6845\n",
      "Epoch 137/500\n",
      "1176/1176 [==============================] - 1s 479us/step - loss: 1.8366 - acc: 0.5740 - val_loss: 1.8011 - val_acc: 0.6865\n",
      "Epoch 138/500\n",
      "1176/1176 [==============================] - 1s 473us/step - loss: 1.7997 - acc: 0.5944 - val_loss: 1.7984 - val_acc: 0.6865\n",
      "Epoch 139/500\n",
      "1176/1176 [==============================] - 1s 478us/step - loss: 1.8428 - acc: 0.5935 - val_loss: 1.7990 - val_acc: 0.6865\n",
      "Epoch 140/500\n",
      "1176/1176 [==============================] - 1s 478us/step - loss: 1.7125 - acc: 0.6259 - val_loss: 1.7687 - val_acc: 0.6786\n",
      "Epoch 141/500\n",
      "1176/1176 [==============================] - 1s 476us/step - loss: 1.7453 - acc: 0.5986 - val_loss: 1.7677 - val_acc: 0.6865\n",
      "Epoch 142/500\n",
      "1176/1176 [==============================] - 1s 471us/step - loss: 1.7909 - acc: 0.5893 - val_loss: 1.7419 - val_acc: 0.6885\n",
      "Epoch 143/500\n",
      "1176/1176 [==============================] - 1s 467us/step - loss: 1.7709 - acc: 0.6114 - val_loss: 1.7503 - val_acc: 0.6905\n",
      "Epoch 144/500\n",
      "1176/1176 [==============================] - 1s 485us/step - loss: 1.7246 - acc: 0.6131 - val_loss: 1.7474 - val_acc: 0.6825\n",
      "Epoch 145/500\n",
      "1176/1176 [==============================] - 1s 468us/step - loss: 1.7900 - acc: 0.6216 - val_loss: 1.7323 - val_acc: 0.6885\n",
      "Epoch 146/500\n",
      "1176/1176 [==============================] - 1s 479us/step - loss: 1.7605 - acc: 0.6199 - val_loss: 1.7176 - val_acc: 0.6905\n",
      "Epoch 147/500\n",
      "1176/1176 [==============================] - 1s 467us/step - loss: 1.7538 - acc: 0.5995 - val_loss: 1.6964 - val_acc: 0.6984\n",
      "Epoch 148/500\n",
      "1176/1176 [==============================] - 1s 477us/step - loss: 1.7192 - acc: 0.6233 - val_loss: 1.6992 - val_acc: 0.6984\n",
      "Epoch 149/500\n",
      "1176/1176 [==============================] - 1s 621us/step - loss: 1.7111 - acc: 0.6352 - val_loss: 1.6833 - val_acc: 0.7024\n",
      "Epoch 150/500\n",
      "1176/1176 [==============================] - 1s 489us/step - loss: 1.6980 - acc: 0.6173 - val_loss: 1.6604 - val_acc: 0.6944\n",
      "Epoch 151/500\n",
      "1176/1176 [==============================] - 1s 567us/step - loss: 1.6089 - acc: 0.6599 - val_loss: 1.6690 - val_acc: 0.7044\n",
      "Epoch 152/500\n",
      "1176/1176 [==============================] - 1s 515us/step - loss: 1.6086 - acc: 0.6259 - val_loss: 1.6598 - val_acc: 0.7044\n",
      "Epoch 153/500\n",
      "1176/1176 [==============================] - 1s 482us/step - loss: 1.6196 - acc: 0.6267 - val_loss: 1.6476 - val_acc: 0.7063\n",
      "Epoch 154/500\n",
      "1176/1176 [==============================] - 1s 533us/step - loss: 1.6819 - acc: 0.6199 - val_loss: 1.6478 - val_acc: 0.7063\n",
      "Epoch 155/500\n",
      "1176/1176 [==============================] - 1s 535us/step - loss: 1.6411 - acc: 0.6267 - val_loss: 1.6445 - val_acc: 0.7063\n",
      "Epoch 156/500\n",
      "1176/1176 [==============================] - 1s 484us/step - loss: 1.5721 - acc: 0.6556 - val_loss: 1.6198 - val_acc: 0.7143\n",
      "Epoch 157/500\n",
      "1176/1176 [==============================] - 1s 509us/step - loss: 1.6103 - acc: 0.6352 - val_loss: 1.5864 - val_acc: 0.7083\n",
      "Epoch 158/500\n",
      "1176/1176 [==============================] - 1s 499us/step - loss: 1.5783 - acc: 0.6607 - val_loss: 1.5937 - val_acc: 0.7063\n",
      "Epoch 159/500\n",
      "1176/1176 [==============================] - 1s 488us/step - loss: 1.5837 - acc: 0.6284 - val_loss: 1.6065 - val_acc: 0.7103\n",
      "Epoch 160/500\n",
      "1176/1176 [==============================] - 1s 569us/step - loss: 1.5477 - acc: 0.6395 - val_loss: 1.5973 - val_acc: 0.7103\n",
      "Epoch 161/500\n",
      "1176/1176 [==============================] - 1s 539us/step - loss: 1.5390 - acc: 0.6539 - val_loss: 1.5776 - val_acc: 0.7163\n",
      "Epoch 162/500\n",
      "1176/1176 [==============================] - 1s 492us/step - loss: 1.5538 - acc: 0.6505 - val_loss: 1.5537 - val_acc: 0.7183\n",
      "Epoch 163/500\n",
      "1176/1176 [==============================] - 1s 483us/step - loss: 1.5323 - acc: 0.6692 - val_loss: 1.5578 - val_acc: 0.7123\n",
      "Epoch 164/500\n",
      "1176/1176 [==============================] - 1s 502us/step - loss: 1.5588 - acc: 0.6556 - val_loss: 1.5397 - val_acc: 0.7163\n",
      "Epoch 165/500\n",
      "1176/1176 [==============================] - 1s 519us/step - loss: 1.5054 - acc: 0.6556 - val_loss: 1.5399 - val_acc: 0.7183\n",
      "Epoch 166/500\n",
      "1176/1176 [==============================] - 1s 513us/step - loss: 1.5044 - acc: 0.6760 - val_loss: 1.5521 - val_acc: 0.7143\n",
      "Epoch 167/500\n",
      "1176/1176 [==============================] - 1s 505us/step - loss: 1.4699 - acc: 0.6862 - val_loss: 1.5393 - val_acc: 0.7163\n",
      "Epoch 168/500\n",
      "1176/1176 [==============================] - 1s 474us/step - loss: 1.5064 - acc: 0.6667 - val_loss: 1.5283 - val_acc: 0.7222\n",
      "Epoch 169/500\n",
      "1176/1176 [==============================] - 1s 481us/step - loss: 1.4749 - acc: 0.6769 - val_loss: 1.5304 - val_acc: 0.7183\n",
      "Epoch 170/500\n",
      "1176/1176 [==============================] - 1s 463us/step - loss: 1.4865 - acc: 0.6803 - val_loss: 1.5150 - val_acc: 0.7262\n",
      "Epoch 171/500\n",
      "1176/1176 [==============================] - 1s 473us/step - loss: 1.5105 - acc: 0.6845 - val_loss: 1.5067 - val_acc: 0.7242\n",
      "Epoch 172/500\n",
      "1176/1176 [==============================] - 1s 480us/step - loss: 1.5196 - acc: 0.6726 - val_loss: 1.5051 - val_acc: 0.7262\n",
      "Epoch 173/500\n",
      "1176/1176 [==============================] - 1s 490us/step - loss: 1.4653 - acc: 0.6735 - val_loss: 1.4802 - val_acc: 0.7401\n",
      "Epoch 174/500\n",
      "1176/1176 [==============================] - 1s 474us/step - loss: 1.4872 - acc: 0.6624 - val_loss: 1.4715 - val_acc: 0.7500\n",
      "Epoch 175/500\n",
      "1176/1176 [==============================] - 1s 485us/step - loss: 1.4621 - acc: 0.6879 - val_loss: 1.4769 - val_acc: 0.7401\n",
      "Epoch 176/500\n",
      "1176/1176 [==============================] - 1s 473us/step - loss: 1.4335 - acc: 0.6820 - val_loss: 1.4626 - val_acc: 0.7560\n",
      "Epoch 177/500\n",
      "1176/1176 [==============================] - 1s 475us/step - loss: 1.3763 - acc: 0.6981 - val_loss: 1.4344 - val_acc: 0.7500\n",
      "Epoch 178/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1176/1176 [==============================] - 1s 478us/step - loss: 1.4505 - acc: 0.6735 - val_loss: 1.4428 - val_acc: 0.7460\n",
      "Epoch 179/500\n",
      "1176/1176 [==============================] - 1s 470us/step - loss: 1.4466 - acc: 0.6854 - val_loss: 1.4348 - val_acc: 0.7361\n",
      "Epoch 180/500\n",
      "1176/1176 [==============================] - 1s 475us/step - loss: 1.4342 - acc: 0.6879 - val_loss: 1.4134 - val_acc: 0.7421\n",
      "Epoch 181/500\n",
      "1176/1176 [==============================] - 1s 480us/step - loss: 1.4008 - acc: 0.6871 - val_loss: 1.4538 - val_acc: 0.7302\n",
      "Epoch 182/500\n",
      "1176/1176 [==============================] - 1s 473us/step - loss: 1.3794 - acc: 0.7007 - val_loss: 1.4459 - val_acc: 0.7401\n",
      "Epoch 183/500\n",
      "1176/1176 [==============================] - 1s 472us/step - loss: 1.3879 - acc: 0.6871 - val_loss: 1.4358 - val_acc: 0.7421\n",
      "Epoch 184/500\n",
      "1176/1176 [==============================] - 1s 475us/step - loss: 1.3672 - acc: 0.7058 - val_loss: 1.4249 - val_acc: 0.7401\n",
      "Epoch 185/500\n",
      "1176/1176 [==============================] - 1s 474us/step - loss: 1.2774 - acc: 0.7194 - val_loss: 1.4175 - val_acc: 0.7341\n",
      "Epoch 186/500\n",
      "1176/1176 [==============================] - 1s 470us/step - loss: 1.3603 - acc: 0.6990 - val_loss: 1.4209 - val_acc: 0.7361\n",
      "Epoch 187/500\n",
      "1176/1176 [==============================] - 1s 475us/step - loss: 1.3675 - acc: 0.6998 - val_loss: 1.4085 - val_acc: 0.7381\n",
      "Epoch 188/500\n",
      "1176/1176 [==============================] - 1s 469us/step - loss: 1.3054 - acc: 0.7134 - val_loss: 1.3786 - val_acc: 0.7421\n",
      "Epoch 189/500\n",
      "1176/1176 [==============================] - 1s 476us/step - loss: 1.2684 - acc: 0.6998 - val_loss: 1.3681 - val_acc: 0.7460\n",
      "Epoch 190/500\n",
      "1176/1176 [==============================] - 1s 481us/step - loss: 1.3202 - acc: 0.6981 - val_loss: 1.3692 - val_acc: 0.7440\n",
      "Epoch 191/500\n",
      "1176/1176 [==============================] - 1s 478us/step - loss: 1.3139 - acc: 0.7041 - val_loss: 1.3636 - val_acc: 0.7480\n",
      "Epoch 192/500\n",
      "1176/1176 [==============================] - 1s 485us/step - loss: 1.3123 - acc: 0.7117 - val_loss: 1.3843 - val_acc: 0.7480\n",
      "Epoch 193/500\n",
      "1176/1176 [==============================] - 1s 488us/step - loss: 1.3112 - acc: 0.7049 - val_loss: 1.3731 - val_acc: 0.7480\n",
      "Epoch 194/500\n",
      "1176/1176 [==============================] - 1s 486us/step - loss: 1.3590 - acc: 0.7202 - val_loss: 1.3790 - val_acc: 0.7560\n",
      "Epoch 195/500\n",
      "1176/1176 [==============================] - 1s 468us/step - loss: 1.3461 - acc: 0.7041 - val_loss: 1.3679 - val_acc: 0.7520\n",
      "Epoch 196/500\n",
      "1176/1176 [==============================] - 1s 480us/step - loss: 1.3276 - acc: 0.7168 - val_loss: 1.3390 - val_acc: 0.7540\n",
      "Epoch 197/500\n",
      "1176/1176 [==============================] - 1s 465us/step - loss: 1.2720 - acc: 0.7083 - val_loss: 1.3170 - val_acc: 0.7639\n",
      "Epoch 198/500\n",
      "1176/1176 [==============================] - 1s 469us/step - loss: 1.3085 - acc: 0.7083 - val_loss: 1.3356 - val_acc: 0.7560\n",
      "Epoch 199/500\n",
      "1176/1176 [==============================] - 1s 476us/step - loss: 1.3321 - acc: 0.7160 - val_loss: 1.3068 - val_acc: 0.7619\n",
      "Epoch 200/500\n",
      "1176/1176 [==============================] - 1s 472us/step - loss: 1.2814 - acc: 0.7168 - val_loss: 1.2982 - val_acc: 0.7619\n",
      "Epoch 201/500\n",
      "1176/1176 [==============================] - 1s 475us/step - loss: 1.2802 - acc: 0.7041 - val_loss: 1.3218 - val_acc: 0.7560\n",
      "Epoch 202/500\n",
      "1176/1176 [==============================] - 1s 470us/step - loss: 1.3681 - acc: 0.6964 - val_loss: 1.2817 - val_acc: 0.7758\n",
      "Epoch 203/500\n",
      "1176/1176 [==============================] - 1s 475us/step - loss: 1.3191 - acc: 0.7202 - val_loss: 1.2853 - val_acc: 0.7639\n",
      "Epoch 204/500\n",
      "1176/1176 [==============================] - 1s 471us/step - loss: 1.2369 - acc: 0.7338 - val_loss: 1.3004 - val_acc: 0.7579\n",
      "Epoch 205/500\n",
      "1176/1176 [==============================] - 1s 468us/step - loss: 1.2928 - acc: 0.7185 - val_loss: 1.3020 - val_acc: 0.7579\n",
      "Epoch 206/500\n",
      "1176/1176 [==============================] - 1s 481us/step - loss: 1.2395 - acc: 0.7364 - val_loss: 1.2798 - val_acc: 0.7500\n",
      "Epoch 207/500\n",
      "1176/1176 [==============================] - 1s 474us/step - loss: 1.1931 - acc: 0.7517 - val_loss: 1.2664 - val_acc: 0.7639\n",
      "Epoch 208/500\n",
      "1176/1176 [==============================] - 1s 467us/step - loss: 1.2814 - acc: 0.7440 - val_loss: 1.2923 - val_acc: 0.7639\n",
      "Epoch 209/500\n",
      "1176/1176 [==============================] - 1s 467us/step - loss: 1.1857 - acc: 0.7415 - val_loss: 1.2550 - val_acc: 0.7758\n",
      "Epoch 210/500\n",
      "1176/1176 [==============================] - 1s 470us/step - loss: 1.1725 - acc: 0.7551 - val_loss: 1.2565 - val_acc: 0.7639\n",
      "Epoch 211/500\n",
      "1176/1176 [==============================] - 1s 478us/step - loss: 1.1452 - acc: 0.7381 - val_loss: 1.2515 - val_acc: 0.7857\n",
      "Epoch 212/500\n",
      "1176/1176 [==============================] - 1s 621us/step - loss: 1.2432 - acc: 0.7245 - val_loss: 1.2430 - val_acc: 0.7758\n",
      "Epoch 213/500\n",
      "1176/1176 [==============================] - 1s 518us/step - loss: 1.2420 - acc: 0.7389 - val_loss: 1.2603 - val_acc: 0.7698\n",
      "Epoch 214/500\n",
      "1176/1176 [==============================] - 1s 564us/step - loss: 1.1982 - acc: 0.7440 - val_loss: 1.2561 - val_acc: 0.7679\n",
      "Epoch 215/500\n",
      "1176/1176 [==============================] - 1s 488us/step - loss: 1.2308 - acc: 0.7355 - val_loss: 1.2560 - val_acc: 0.7758\n",
      "Epoch 216/500\n",
      "1176/1176 [==============================] - 1s 484us/step - loss: 1.1330 - acc: 0.7551 - val_loss: 1.2237 - val_acc: 0.7817\n",
      "Epoch 217/500\n",
      "1176/1176 [==============================] - 1s 475us/step - loss: 1.1264 - acc: 0.7509 - val_loss: 1.2291 - val_acc: 0.7837\n",
      "Epoch 218/500\n",
      "1176/1176 [==============================] - 1s 483us/step - loss: 1.2717 - acc: 0.7236 - val_loss: 1.2156 - val_acc: 0.7837\n",
      "Epoch 219/500\n",
      "1176/1176 [==============================] - 1s 554us/step - loss: 1.2712 - acc: 0.7457 - val_loss: 1.2414 - val_acc: 0.7937\n",
      "Epoch 220/500\n",
      "1176/1176 [==============================] - 1s 504us/step - loss: 1.1706 - acc: 0.7330 - val_loss: 1.2286 - val_acc: 0.7937\n",
      "Epoch 221/500\n",
      "1176/1176 [==============================] - 1s 488us/step - loss: 1.1681 - acc: 0.7509 - val_loss: 1.2140 - val_acc: 0.7917\n",
      "Epoch 222/500\n",
      "1176/1176 [==============================] - 1s 528us/step - loss: 1.1935 - acc: 0.7457 - val_loss: 1.2217 - val_acc: 0.8056\n",
      "Epoch 223/500\n",
      "1176/1176 [==============================] - 1s 589us/step - loss: 1.1217 - acc: 0.7543 - val_loss: 1.2033 - val_acc: 0.7976\n",
      "Epoch 224/500\n",
      "1176/1176 [==============================] - 1s 540us/step - loss: 1.1899 - acc: 0.7262 - val_loss: 1.1883 - val_acc: 0.7897\n",
      "Epoch 225/500\n",
      "1176/1176 [==============================] - 1s 486us/step - loss: 1.2193 - acc: 0.7364 - val_loss: 1.1928 - val_acc: 0.7837\n",
      "Epoch 226/500\n",
      "1176/1176 [==============================] - 1s 475us/step - loss: 1.2385 - acc: 0.7270 - val_loss: 1.2007 - val_acc: 0.7917\n",
      "Epoch 227/500\n",
      "1176/1176 [==============================] - 1s 512us/step - loss: 1.1697 - acc: 0.7619 - val_loss: 1.1867 - val_acc: 0.7917\n",
      "Epoch 228/500\n",
      "1176/1176 [==============================] - 1s 521us/step - loss: 1.0816 - acc: 0.7619 - val_loss: 1.1832 - val_acc: 0.7956\n",
      "Epoch 229/500\n",
      "1176/1176 [==============================] - 1s 513us/step - loss: 1.1586 - acc: 0.7662 - val_loss: 1.1445 - val_acc: 0.8036\n",
      "Epoch 230/500\n",
      "1176/1176 [==============================] - 1s 501us/step - loss: 1.1578 - acc: 0.7653 - val_loss: 1.1688 - val_acc: 0.7897\n",
      "Epoch 231/500\n",
      "1176/1176 [==============================] - 1s 468us/step - loss: 1.1388 - acc: 0.7406 - val_loss: 1.1579 - val_acc: 0.8016\n",
      "Epoch 232/500\n",
      "1176/1176 [==============================] - 1s 483us/step - loss: 1.1027 - acc: 0.7619 - val_loss: 1.1566 - val_acc: 0.7956\n",
      "Epoch 233/500\n",
      "1176/1176 [==============================] - 1s 475us/step - loss: 1.1763 - acc: 0.7619 - val_loss: 1.1634 - val_acc: 0.7917\n",
      "Epoch 234/500\n",
      "1176/1176 [==============================] - 1s 471us/step - loss: 1.1311 - acc: 0.7721 - val_loss: 1.1307 - val_acc: 0.8036\n",
      "Epoch 235/500\n",
      "1176/1176 [==============================] - 1s 476us/step - loss: 1.1104 - acc: 0.7721 - val_loss: 1.1276 - val_acc: 0.8075\n",
      "Epoch 236/500\n",
      "1176/1176 [==============================] - 1s 480us/step - loss: 1.1270 - acc: 0.7534 - val_loss: 1.1485 - val_acc: 0.8095\n",
      "Epoch 237/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1176/1176 [==============================] - 1s 476us/step - loss: 1.0839 - acc: 0.7653 - val_loss: 1.1142 - val_acc: 0.8135\n",
      "Epoch 238/500\n",
      "1176/1176 [==============================] - 1s 469us/step - loss: 1.1243 - acc: 0.7500 - val_loss: 1.0937 - val_acc: 0.8175\n",
      "Epoch 239/500\n",
      "1176/1176 [==============================] - 1s 465us/step - loss: 1.0826 - acc: 0.7636 - val_loss: 1.0987 - val_acc: 0.8056\n",
      "Epoch 240/500\n",
      "1176/1176 [==============================] - 1s 469us/step - loss: 1.1305 - acc: 0.7483 - val_loss: 1.1256 - val_acc: 0.8075\n",
      "Epoch 241/500\n",
      "1176/1176 [==============================] - 1s 480us/step - loss: 1.1524 - acc: 0.7636 - val_loss: 1.0990 - val_acc: 0.8095\n",
      "Epoch 242/500\n",
      "1176/1176 [==============================] - 1s 611us/step - loss: 1.0428 - acc: 0.7823 - val_loss: 1.0924 - val_acc: 0.8115\n",
      "Epoch 243/500\n",
      "1176/1176 [==============================] - 1s 502us/step - loss: 1.1385 - acc: 0.7551 - val_loss: 1.0576 - val_acc: 0.8175\n",
      "Epoch 244/500\n",
      "1176/1176 [==============================] - 1s 514us/step - loss: 1.0286 - acc: 0.7747 - val_loss: 1.0901 - val_acc: 0.8135\n",
      "Epoch 245/500\n",
      "1176/1176 [==============================] - 1s 482us/step - loss: 1.0574 - acc: 0.7679 - val_loss: 1.0808 - val_acc: 0.8155\n",
      "Epoch 246/500\n",
      "1176/1176 [==============================] - 1s 478us/step - loss: 1.1043 - acc: 0.7568 - val_loss: 1.0607 - val_acc: 0.8234\n",
      "Epoch 247/500\n",
      "1176/1176 [==============================] - 1s 476us/step - loss: 1.0029 - acc: 0.7883 - val_loss: 1.1087 - val_acc: 0.8075\n",
      "Epoch 248/500\n",
      "1176/1176 [==============================] - 1s 525us/step - loss: 1.0543 - acc: 0.7679 - val_loss: 1.1203 - val_acc: 0.8095\n",
      "Epoch 249/500\n",
      "1176/1176 [==============================] - 1s 545us/step - loss: 1.0942 - acc: 0.7619 - val_loss: 1.1256 - val_acc: 0.8135\n",
      "Epoch 250/500\n",
      "1176/1176 [==============================] - 1s 546us/step - loss: 1.0869 - acc: 0.7781 - val_loss: 1.1316 - val_acc: 0.8155\n",
      "Epoch 251/500\n",
      "1176/1176 [==============================] - 1s 495us/step - loss: 1.0356 - acc: 0.7721 - val_loss: 1.0848 - val_acc: 0.8155\n",
      "Epoch 252/500\n",
      "1176/1176 [==============================] - 1s 474us/step - loss: 0.9969 - acc: 0.7857 - val_loss: 1.1127 - val_acc: 0.8135\n",
      "Epoch 253/500\n",
      "1176/1176 [==============================] - 1s 476us/step - loss: 1.1100 - acc: 0.7721 - val_loss: 1.0802 - val_acc: 0.8194\n",
      "Epoch 254/500\n",
      "1176/1176 [==============================] - 1s 538us/step - loss: 1.0614 - acc: 0.7704 - val_loss: 1.0713 - val_acc: 0.8135\n",
      "Epoch 255/500\n",
      "1176/1176 [==============================] - 1s 502us/step - loss: 1.0358 - acc: 0.7721 - val_loss: 1.0484 - val_acc: 0.8175\n",
      "Epoch 256/500\n",
      "1176/1176 [==============================] - 1s 502us/step - loss: 1.0391 - acc: 0.7764 - val_loss: 1.0546 - val_acc: 0.8155\n",
      "Epoch 257/500\n",
      "1176/1176 [==============================] - 1s 486us/step - loss: 0.9922 - acc: 0.7789 - val_loss: 1.0702 - val_acc: 0.8274\n",
      "Epoch 258/500\n",
      "1176/1176 [==============================] - 1s 475us/step - loss: 1.0060 - acc: 0.7840 - val_loss: 1.0287 - val_acc: 0.8214\n",
      "Epoch 259/500\n",
      "1176/1176 [==============================] - 1s 483us/step - loss: 1.0702 - acc: 0.7747 - val_loss: 1.0018 - val_acc: 0.8333\n",
      "Epoch 260/500\n",
      "1176/1176 [==============================] - 1s 483us/step - loss: 1.0655 - acc: 0.7738 - val_loss: 1.0459 - val_acc: 0.8274\n",
      "Epoch 261/500\n",
      "1176/1176 [==============================] - 1s 479us/step - loss: 0.9576 - acc: 0.7908 - val_loss: 1.0592 - val_acc: 0.8234\n",
      "Epoch 262/500\n",
      "1176/1176 [==============================] - 1s 461us/step - loss: 1.0598 - acc: 0.7883 - val_loss: 1.0295 - val_acc: 0.8333\n",
      "Epoch 263/500\n",
      "1176/1176 [==============================] - 1s 479us/step - loss: 1.0710 - acc: 0.7653 - val_loss: 1.0329 - val_acc: 0.8373\n",
      "Epoch 264/500\n",
      "1176/1176 [==============================] - 1s 477us/step - loss: 1.0264 - acc: 0.7823 - val_loss: 1.0144 - val_acc: 0.8294\n",
      "Epoch 265/500\n",
      "1176/1176 [==============================] - 1s 473us/step - loss: 1.0183 - acc: 0.7738 - val_loss: 1.0281 - val_acc: 0.8254\n",
      "Epoch 266/500\n",
      "1176/1176 [==============================] - 1s 461us/step - loss: 1.0268 - acc: 0.7815 - val_loss: 1.0074 - val_acc: 0.8234\n",
      "Epoch 267/500\n",
      "1176/1176 [==============================] - 1s 474us/step - loss: 0.9778 - acc: 0.7823 - val_loss: 1.0278 - val_acc: 0.8333\n",
      "Epoch 268/500\n",
      "1176/1176 [==============================] - 1s 479us/step - loss: 1.0156 - acc: 0.7883 - val_loss: 1.0172 - val_acc: 0.8373\n",
      "Epoch 269/500\n",
      "1176/1176 [==============================] - 1s 467us/step - loss: 0.9718 - acc: 0.7968 - val_loss: 1.0202 - val_acc: 0.8333\n",
      "Epoch 270/500\n",
      "1176/1176 [==============================] - 1s 477us/step - loss: 1.0234 - acc: 0.7738 - val_loss: 0.9763 - val_acc: 0.8313\n",
      "Epoch 271/500\n",
      "1176/1176 [==============================] - 1s 470us/step - loss: 0.9694 - acc: 0.7959 - val_loss: 0.9849 - val_acc: 0.8333\n",
      "Epoch 272/500\n",
      "1176/1176 [==============================] - 1s 476us/step - loss: 1.0333 - acc: 0.7798 - val_loss: 1.0216 - val_acc: 0.8353\n",
      "Epoch 273/500\n",
      "1176/1176 [==============================] - 1s 475us/step - loss: 0.9305 - acc: 0.7993 - val_loss: 1.0335 - val_acc: 0.8413\n",
      "Epoch 274/500\n",
      "1176/1176 [==============================] - 1s 470us/step - loss: 1.0138 - acc: 0.7883 - val_loss: 0.9978 - val_acc: 0.8353\n",
      "Epoch 275/500\n",
      "1176/1176 [==============================] - 1s 545us/step - loss: 0.9627 - acc: 0.7755 - val_loss: 1.0148 - val_acc: 0.8472\n",
      "Epoch 276/500\n",
      "1176/1176 [==============================] - 1s 576us/step - loss: 1.0235 - acc: 0.7806 - val_loss: 1.0155 - val_acc: 0.8452\n",
      "Epoch 277/500\n",
      "1176/1176 [==============================] - 1s 536us/step - loss: 0.9217 - acc: 0.8053 - val_loss: 0.9800 - val_acc: 0.8492\n",
      "Epoch 278/500\n",
      "1176/1176 [==============================] - 1s 509us/step - loss: 0.9935 - acc: 0.7840 - val_loss: 0.9873 - val_acc: 0.8452\n",
      "Epoch 279/500\n",
      "1176/1176 [==============================] - 1s 522us/step - loss: 0.9790 - acc: 0.8019 - val_loss: 1.0018 - val_acc: 0.8413\n",
      "Epoch 280/500\n",
      "1176/1176 [==============================] - 1s 516us/step - loss: 0.9947 - acc: 0.8044 - val_loss: 1.0101 - val_acc: 0.8433\n",
      "Epoch 281/500\n",
      "1176/1176 [==============================] - 1s 576us/step - loss: 0.9578 - acc: 0.7866 - val_loss: 1.0031 - val_acc: 0.8452\n",
      "Epoch 282/500\n",
      "1176/1176 [==============================] - 1s 511us/step - loss: 0.9287 - acc: 0.8002 - val_loss: 1.0018 - val_acc: 0.8452\n",
      "Epoch 283/500\n",
      "1176/1176 [==============================] - 1s 489us/step - loss: 0.9842 - acc: 0.7951 - val_loss: 1.0000 - val_acc: 0.8472\n",
      "Epoch 284/500\n",
      "1176/1176 [==============================] - 1s 474us/step - loss: 0.9463 - acc: 0.8010 - val_loss: 1.0017 - val_acc: 0.8433\n",
      "Epoch 285/500\n",
      "1176/1176 [==============================] - 1s 520us/step - loss: 0.9244 - acc: 0.7976 - val_loss: 0.9802 - val_acc: 0.8512\n",
      "Epoch 286/500\n",
      "1176/1176 [==============================] - 1s 501us/step - loss: 0.9439 - acc: 0.7934 - val_loss: 0.9858 - val_acc: 0.8393\n",
      "Epoch 287/500\n",
      "1176/1176 [==============================] - 1s 485us/step - loss: 0.9430 - acc: 0.8044 - val_loss: 0.9881 - val_acc: 0.8433\n",
      "Epoch 288/500\n",
      "1176/1176 [==============================] - 1s 482us/step - loss: 0.9112 - acc: 0.7951 - val_loss: 0.9900 - val_acc: 0.8492\n",
      "Epoch 289/500\n",
      "1176/1176 [==============================] - 1s 475us/step - loss: 0.9961 - acc: 0.8061 - val_loss: 0.9814 - val_acc: 0.8532\n",
      "Epoch 290/500\n",
      "1176/1176 [==============================] - 1s 469us/step - loss: 0.9193 - acc: 0.8044 - val_loss: 1.0091 - val_acc: 0.8472\n",
      "Epoch 291/500\n",
      "1176/1176 [==============================] - 1s 467us/step - loss: 0.9529 - acc: 0.7976 - val_loss: 0.9886 - val_acc: 0.8512\n",
      "Epoch 292/500\n",
      "1176/1176 [==============================] - 1s 473us/step - loss: 0.8863 - acc: 0.8231 - val_loss: 0.9930 - val_acc: 0.8492\n",
      "Epoch 293/500\n",
      "1176/1176 [==============================] - 1s 480us/step - loss: 0.9552 - acc: 0.8078 - val_loss: 0.9675 - val_acc: 0.8532\n",
      "Epoch 294/500\n",
      "1176/1176 [==============================] - 1s 477us/step - loss: 0.9279 - acc: 0.8078 - val_loss: 0.9658 - val_acc: 0.8452\n",
      "Epoch 295/500\n",
      "1176/1176 [==============================] - 1s 475us/step - loss: 0.9010 - acc: 0.8061 - val_loss: 0.9399 - val_acc: 0.8413\n",
      "Epoch 296/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1176/1176 [==============================] - 1s 477us/step - loss: 0.9329 - acc: 0.8027 - val_loss: 0.9294 - val_acc: 0.8492\n",
      "Epoch 297/500\n",
      "1176/1176 [==============================] - 1s 477us/step - loss: 0.8960 - acc: 0.7968 - val_loss: 0.9436 - val_acc: 0.8472\n",
      "Epoch 298/500\n",
      "1176/1176 [==============================] - 1s 482us/step - loss: 0.9257 - acc: 0.8036 - val_loss: 0.9430 - val_acc: 0.8472\n",
      "Epoch 299/500\n",
      "1176/1176 [==============================] - 1s 474us/step - loss: 0.9173 - acc: 0.8044 - val_loss: 0.9310 - val_acc: 0.8452\n",
      "Epoch 300/500\n",
      "1176/1176 [==============================] - 1s 466us/step - loss: 0.9235 - acc: 0.8078 - val_loss: 0.9766 - val_acc: 0.8512\n",
      "Epoch 301/500\n",
      "1176/1176 [==============================] - 1s 465us/step - loss: 0.9175 - acc: 0.8155 - val_loss: 0.9520 - val_acc: 0.8512\n",
      "Epoch 302/500\n",
      "1176/1176 [==============================] - 1s 477us/step - loss: 0.9299 - acc: 0.8180 - val_loss: 0.9230 - val_acc: 0.8512\n",
      "Epoch 303/500\n",
      "1176/1176 [==============================] - 1s 471us/step - loss: 0.9008 - acc: 0.8197 - val_loss: 0.9517 - val_acc: 0.8452\n",
      "Epoch 304/500\n",
      "1176/1176 [==============================] - 1s 464us/step - loss: 0.9163 - acc: 0.8053 - val_loss: 0.9286 - val_acc: 0.8472\n",
      "Epoch 305/500\n",
      "1176/1176 [==============================] - 1s 471us/step - loss: 0.9036 - acc: 0.8095 - val_loss: 0.9266 - val_acc: 0.8512\n",
      "Epoch 306/500\n",
      "1176/1176 [==============================] - 1s 463us/step - loss: 0.8828 - acc: 0.8223 - val_loss: 0.9595 - val_acc: 0.8452\n",
      "Epoch 307/500\n",
      "1176/1176 [==============================] - 1s 468us/step - loss: 0.9402 - acc: 0.8019 - val_loss: 0.9457 - val_acc: 0.8472\n",
      "Epoch 308/500\n",
      "1176/1176 [==============================] - 1s 579us/step - loss: 0.9200 - acc: 0.8053 - val_loss: 0.9123 - val_acc: 0.8492\n",
      "Epoch 309/500\n",
      "1176/1176 [==============================] - 1s 487us/step - loss: 0.9395 - acc: 0.8036 - val_loss: 0.9150 - val_acc: 0.8492\n",
      "Epoch 310/500\n",
      "1176/1176 [==============================] - 1s 527us/step - loss: 0.8940 - acc: 0.8129 - val_loss: 0.9486 - val_acc: 0.8552\n",
      "Epoch 311/500\n",
      "1176/1176 [==============================] - 1s 489us/step - loss: 0.8786 - acc: 0.8240 - val_loss: 0.9585 - val_acc: 0.8512\n",
      "Epoch 312/500\n",
      "1176/1176 [==============================] - 1s 508us/step - loss: 0.9398 - acc: 0.8189 - val_loss: 0.9549 - val_acc: 0.8512\n",
      "Epoch 313/500\n",
      "1176/1176 [==============================] - 1s 466us/step - loss: 0.8761 - acc: 0.8019 - val_loss: 0.9432 - val_acc: 0.8532\n",
      "Epoch 314/500\n",
      "1176/1176 [==============================] - 1s 526us/step - loss: 0.8757 - acc: 0.8129 - val_loss: 0.9560 - val_acc: 0.8512\n",
      "Epoch 315/500\n",
      "1176/1176 [==============================] - 1s 487us/step - loss: 0.9037 - acc: 0.8163 - val_loss: 0.9669 - val_acc: 0.8532\n",
      "Epoch 316/500\n",
      "1176/1176 [==============================] - 1s 574us/step - loss: 0.9309 - acc: 0.8155 - val_loss: 0.9555 - val_acc: 0.8512\n",
      "Epoch 317/500\n",
      "1176/1176 [==============================] - 1s 535us/step - loss: 0.9621 - acc: 0.7968 - val_loss: 0.9600 - val_acc: 0.8532\n",
      "Epoch 318/500\n",
      "1176/1176 [==============================] - 1s 582us/step - loss: 0.8461 - acc: 0.8240 - val_loss: 0.9442 - val_acc: 0.8492\n",
      "Epoch 319/500\n",
      "1176/1176 [==============================] - 1s 569us/step - loss: 0.8976 - acc: 0.8129 - val_loss: 0.9572 - val_acc: 0.8571\n",
      "Epoch 320/500\n",
      "1176/1176 [==============================] - 1s 515us/step - loss: 0.8298 - acc: 0.8197 - val_loss: 0.9255 - val_acc: 0.8611\n",
      "Epoch 321/500\n",
      "1176/1176 [==============================] - 1s 505us/step - loss: 0.8970 - acc: 0.8044 - val_loss: 0.9307 - val_acc: 0.8552\n",
      "Epoch 322/500\n",
      "1176/1176 [==============================] - 1s 469us/step - loss: 0.9135 - acc: 0.8180 - val_loss: 0.9283 - val_acc: 0.8512\n",
      "Epoch 323/500\n",
      "1176/1176 [==============================] - 1s 509us/step - loss: 0.8858 - acc: 0.8087 - val_loss: 0.9255 - val_acc: 0.8591\n",
      "Epoch 324/500\n",
      "1176/1176 [==============================] - 1s 582us/step - loss: 0.8183 - acc: 0.8282 - val_loss: 0.9238 - val_acc: 0.8552\n",
      "Epoch 325/500\n",
      "1176/1176 [==============================] - 1s 479us/step - loss: 0.9350 - acc: 0.8121 - val_loss: 0.9249 - val_acc: 0.8571\n",
      "Epoch 326/500\n",
      "1176/1176 [==============================] - 1s 524us/step - loss: 0.9460 - acc: 0.8036 - val_loss: 0.9250 - val_acc: 0.8631\n",
      "Epoch 327/500\n",
      "1176/1176 [==============================] - 1s 459us/step - loss: 0.8357 - acc: 0.8197 - val_loss: 0.9333 - val_acc: 0.8532\n",
      "Epoch 328/500\n",
      "1176/1176 [==============================] - 1s 473us/step - loss: 0.8495 - acc: 0.8172 - val_loss: 0.9069 - val_acc: 0.8631\n",
      "Epoch 329/500\n",
      "1176/1176 [==============================] - 1s 502us/step - loss: 0.9508 - acc: 0.8061 - val_loss: 0.9154 - val_acc: 0.8552\n",
      "Epoch 330/500\n",
      "1176/1176 [==============================] - 1s 513us/step - loss: 0.8382 - acc: 0.8172 - val_loss: 0.9182 - val_acc: 0.8571\n",
      "Epoch 331/500\n",
      "1176/1176 [==============================] - 1s 495us/step - loss: 0.8492 - acc: 0.8231 - val_loss: 0.9046 - val_acc: 0.8571\n",
      "Epoch 332/500\n",
      "1176/1176 [==============================] - 1s 490us/step - loss: 0.8658 - acc: 0.8299 - val_loss: 0.9068 - val_acc: 0.8611\n",
      "Epoch 333/500\n",
      "1176/1176 [==============================] - 1s 471us/step - loss: 0.9528 - acc: 0.8206 - val_loss: 0.8866 - val_acc: 0.8611\n",
      "Epoch 334/500\n",
      "1176/1176 [==============================] - 1s 470us/step - loss: 0.7752 - acc: 0.8129 - val_loss: 0.8327 - val_acc: 0.8671\n",
      "Epoch 335/500\n",
      "1176/1176 [==============================] - 1s 485us/step - loss: 0.8277 - acc: 0.8274 - val_loss: 0.8430 - val_acc: 0.8571\n",
      "Epoch 336/500\n",
      "1176/1176 [==============================] - 1s 481us/step - loss: 0.8094 - acc: 0.8282 - val_loss: 0.8771 - val_acc: 0.8552\n",
      "Epoch 337/500\n",
      "1176/1176 [==============================] - 1s 478us/step - loss: 0.8497 - acc: 0.8401 - val_loss: 0.9022 - val_acc: 0.8571\n",
      "Epoch 338/500\n",
      "1176/1176 [==============================] - 1s 488us/step - loss: 0.9289 - acc: 0.8146 - val_loss: 0.8608 - val_acc: 0.8631\n",
      "Epoch 339/500\n",
      "1176/1176 [==============================] - 1s 485us/step - loss: 0.7944 - acc: 0.8325 - val_loss: 0.8840 - val_acc: 0.8611\n",
      "Epoch 340/500\n",
      "1176/1176 [==============================] - 1s 470us/step - loss: 0.8397 - acc: 0.8240 - val_loss: 0.9260 - val_acc: 0.8651\n",
      "Epoch 341/500\n",
      "1176/1176 [==============================] - 1s 478us/step - loss: 0.8607 - acc: 0.8189 - val_loss: 0.9102 - val_acc: 0.8532\n",
      "Epoch 342/500\n",
      "1176/1176 [==============================] - 1s 472us/step - loss: 0.8382 - acc: 0.8180 - val_loss: 0.9197 - val_acc: 0.8492\n",
      "Epoch 343/500\n",
      "1176/1176 [==============================] - 1s 479us/step - loss: 0.8611 - acc: 0.8214 - val_loss: 0.8872 - val_acc: 0.8552\n",
      "Epoch 344/500\n",
      "1176/1176 [==============================] - 1s 473us/step - loss: 0.9017 - acc: 0.8410 - val_loss: 0.8611 - val_acc: 0.8631\n",
      "Epoch 345/500\n",
      "1176/1176 [==============================] - 1s 479us/step - loss: 0.8874 - acc: 0.8197 - val_loss: 0.8566 - val_acc: 0.8690\n",
      "Epoch 346/500\n",
      "1176/1176 [==============================] - 1s 465us/step - loss: 0.8834 - acc: 0.8155 - val_loss: 0.8363 - val_acc: 0.8690\n",
      "Epoch 347/500\n",
      "1176/1176 [==============================] - 1s 471us/step - loss: 0.7325 - acc: 0.8452 - val_loss: 0.8433 - val_acc: 0.8651\n",
      "Epoch 348/500\n",
      "1176/1176 [==============================] - 1s 469us/step - loss: 0.8151 - acc: 0.8265 - val_loss: 0.8741 - val_acc: 0.8591\n",
      "Epoch 349/500\n",
      "1176/1176 [==============================] - 1s 464us/step - loss: 0.8436 - acc: 0.8316 - val_loss: 0.8857 - val_acc: 0.8631\n",
      "Epoch 350/500\n",
      "1176/1176 [==============================] - 1s 474us/step - loss: 0.8038 - acc: 0.8393 - val_loss: 0.8932 - val_acc: 0.8552\n",
      "Epoch 351/500\n",
      "1176/1176 [==============================] - 1s 484us/step - loss: 0.8479 - acc: 0.8265 - val_loss: 0.8836 - val_acc: 0.8671\n",
      "Epoch 352/500\n",
      "1176/1176 [==============================] - 1s 461us/step - loss: 0.7515 - acc: 0.8401 - val_loss: 0.8770 - val_acc: 0.8671\n",
      "Epoch 353/500\n",
      "1176/1176 [==============================] - 1s 471us/step - loss: 0.8032 - acc: 0.8274 - val_loss: 0.8818 - val_acc: 0.8611\n",
      "Epoch 354/500\n",
      "1176/1176 [==============================] - 1s 476us/step - loss: 0.8026 - acc: 0.8452 - val_loss: 0.8741 - val_acc: 0.8631\n",
      "Epoch 355/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1176/1176 [==============================] - 1s 479us/step - loss: 0.8428 - acc: 0.8350 - val_loss: 0.8963 - val_acc: 0.8611\n",
      "Epoch 356/500\n",
      "1176/1176 [==============================] - 1s 475us/step - loss: 0.8294 - acc: 0.8342 - val_loss: 0.8862 - val_acc: 0.8591\n",
      "Epoch 357/500\n",
      "1176/1176 [==============================] - 1s 474us/step - loss: 0.8250 - acc: 0.8308 - val_loss: 0.8711 - val_acc: 0.8651\n",
      "Epoch 358/500\n",
      "1176/1176 [==============================] - 1s 482us/step - loss: 0.8162 - acc: 0.8206 - val_loss: 0.8646 - val_acc: 0.8710\n",
      "Epoch 359/500\n",
      "1176/1176 [==============================] - 1s 478us/step - loss: 0.8546 - acc: 0.8308 - val_loss: 0.8686 - val_acc: 0.8730\n",
      "Epoch 360/500\n",
      "1176/1176 [==============================] - 1s 484us/step - loss: 0.8982 - acc: 0.8418 - val_loss: 0.9076 - val_acc: 0.8591\n",
      "Epoch 361/500\n",
      "1176/1176 [==============================] - 1s 479us/step - loss: 0.7820 - acc: 0.8418 - val_loss: 0.9037 - val_acc: 0.8651\n",
      "Epoch 362/500\n",
      "1176/1176 [==============================] - 1s 476us/step - loss: 0.7870 - acc: 0.8393 - val_loss: 0.8905 - val_acc: 0.8631\n",
      "Epoch 363/500\n",
      "1176/1176 [==============================] - 1s 465us/step - loss: 0.8163 - acc: 0.8367 - val_loss: 0.9053 - val_acc: 0.8651\n",
      "Epoch 364/500\n",
      "1176/1176 [==============================] - 1s 469us/step - loss: 0.7680 - acc: 0.8410 - val_loss: 0.8817 - val_acc: 0.8651\n",
      "Epoch 365/500\n",
      "1176/1176 [==============================] - 1s 475us/step - loss: 0.7962 - acc: 0.8333 - val_loss: 0.8331 - val_acc: 0.8671\n",
      "Epoch 366/500\n",
      "1176/1176 [==============================] - 1s 576us/step - loss: 0.7676 - acc: 0.8444 - val_loss: 0.8599 - val_acc: 0.8671\n",
      "Epoch 367/500\n",
      "1176/1176 [==============================] - 1s 508us/step - loss: 0.8156 - acc: 0.8333 - val_loss: 0.8714 - val_acc: 0.8651\n",
      "Epoch 368/500\n",
      "1176/1176 [==============================] - 1s 540us/step - loss: 0.8656 - acc: 0.8291 - val_loss: 0.8859 - val_acc: 0.8651\n",
      "Epoch 369/500\n",
      "1176/1176 [==============================] - 1s 486us/step - loss: 0.7485 - acc: 0.8503 - val_loss: 0.8853 - val_acc: 0.8631\n",
      "Epoch 370/500\n",
      "1176/1176 [==============================] - 1s 580us/step - loss: 0.8217 - acc: 0.8393 - val_loss: 0.8613 - val_acc: 0.8651\n",
      "Epoch 371/500\n",
      "1176/1176 [==============================] - 1s 525us/step - loss: 0.7932 - acc: 0.8478 - val_loss: 0.8657 - val_acc: 0.8631\n",
      "Epoch 372/500\n",
      "1176/1176 [==============================] - 1s 503us/step - loss: 0.7817 - acc: 0.8342 - val_loss: 0.8444 - val_acc: 0.8690\n",
      "Epoch 373/500\n",
      "1176/1176 [==============================] - 1s 476us/step - loss: 0.8142 - acc: 0.8325 - val_loss: 0.8697 - val_acc: 0.8690\n",
      "Epoch 374/500\n",
      "1176/1176 [==============================] - 1s 501us/step - loss: 0.8219 - acc: 0.8384 - val_loss: 0.8527 - val_acc: 0.8651\n",
      "Epoch 375/500\n",
      "1176/1176 [==============================] - 1s 524us/step - loss: 0.7853 - acc: 0.8410 - val_loss: 0.8753 - val_acc: 0.8671\n",
      "Epoch 376/500\n",
      "1176/1176 [==============================] - 1s 480us/step - loss: 0.7994 - acc: 0.8427 - val_loss: 0.8649 - val_acc: 0.8690\n",
      "Epoch 377/500\n",
      "1176/1176 [==============================] - 1s 501us/step - loss: 0.7686 - acc: 0.8384 - val_loss: 0.8497 - val_acc: 0.8651\n",
      "Epoch 378/500\n",
      "1176/1176 [==============================] - ETA: 0s - loss: 0.7964 - acc: 0.827 - 1s 483us/step - loss: 0.8195 - acc: 0.8257 - val_loss: 0.8315 - val_acc: 0.8651\n",
      "Epoch 379/500\n",
      "1176/1176 [==============================] - 1s 466us/step - loss: 0.8121 - acc: 0.8316 - val_loss: 0.8102 - val_acc: 0.8750\n",
      "Epoch 380/500\n",
      "1176/1176 [==============================] - 1s 543us/step - loss: 0.7824 - acc: 0.8350 - val_loss: 0.8138 - val_acc: 0.8651\n",
      "Epoch 381/500\n",
      "1176/1176 [==============================] - 1s 545us/step - loss: 0.8047 - acc: 0.8418 - val_loss: 0.8359 - val_acc: 0.8710\n",
      "Epoch 382/500\n",
      "1176/1176 [==============================] - 1s 491us/step - loss: 0.8061 - acc: 0.8418 - val_loss: 0.7931 - val_acc: 0.8770\n",
      "Epoch 383/500\n",
      "1176/1176 [==============================] - 1s 523us/step - loss: 0.7428 - acc: 0.8350 - val_loss: 0.8033 - val_acc: 0.8730\n",
      "Epoch 384/500\n",
      "1176/1176 [==============================] - 1s 483us/step - loss: 0.7424 - acc: 0.8461 - val_loss: 0.7960 - val_acc: 0.8730\n",
      "Epoch 385/500\n",
      "1176/1176 [==============================] - 1s 497us/step - loss: 0.7776 - acc: 0.8512 - val_loss: 0.7992 - val_acc: 0.8690\n",
      "Epoch 386/500\n",
      "1176/1176 [==============================] - 1s 522us/step - loss: 0.8013 - acc: 0.8333 - val_loss: 0.8054 - val_acc: 0.8730\n",
      "Epoch 387/500\n",
      "1176/1176 [==============================] - 0s 424us/step - loss: 0.8545 - acc: 0.8282 - val_loss: 0.8229 - val_acc: 0.8770\n",
      "Epoch 388/500\n",
      "1176/1176 [==============================] - 1s 488us/step - loss: 0.7793 - acc: 0.8197 - val_loss: 0.8238 - val_acc: 0.8710\n",
      "Epoch 389/500\n",
      "1176/1176 [==============================] - 1s 497us/step - loss: 0.8134 - acc: 0.8299 - val_loss: 0.8367 - val_acc: 0.8591\n",
      "Epoch 390/500\n",
      "1176/1176 [==============================] - 1s 495us/step - loss: 0.7945 - acc: 0.8359 - val_loss: 0.8162 - val_acc: 0.8671\n",
      "Epoch 391/500\n",
      "1176/1176 [==============================] - 1s 505us/step - loss: 0.8213 - acc: 0.8427 - val_loss: 0.8341 - val_acc: 0.8631\n",
      "Epoch 392/500\n",
      "1176/1176 [==============================] - 1s 512us/step - loss: 0.7586 - acc: 0.8469 - val_loss: 0.8165 - val_acc: 0.8671\n",
      "Epoch 393/500\n",
      "1176/1176 [==============================] - 1s 666us/step - loss: 0.7109 - acc: 0.8435 - val_loss: 0.8035 - val_acc: 0.8690\n",
      "Epoch 394/500\n",
      "1176/1176 [==============================] - 1s 534us/step - loss: 0.7902 - acc: 0.8359 - val_loss: 0.8465 - val_acc: 0.8651\n",
      "Epoch 395/500\n",
      "1176/1176 [==============================] - 1s 521us/step - loss: 0.7531 - acc: 0.8495 - val_loss: 0.8207 - val_acc: 0.8671\n",
      "Epoch 396/500\n",
      "1176/1176 [==============================] - 1s 578us/step - loss: 0.7601 - acc: 0.8452 - val_loss: 0.8215 - val_acc: 0.8690\n",
      "Epoch 397/500\n",
      "1176/1176 [==============================] - 1s 525us/step - loss: 0.7068 - acc: 0.8461 - val_loss: 0.8256 - val_acc: 0.8671\n",
      "Epoch 398/500\n",
      "1176/1176 [==============================] - 1s 479us/step - loss: 0.7838 - acc: 0.8554 - val_loss: 0.8541 - val_acc: 0.8690\n",
      "Epoch 399/500\n",
      "1176/1176 [==============================] - 1s 482us/step - loss: 0.7668 - acc: 0.8435 - val_loss: 0.8312 - val_acc: 0.8651\n",
      "Epoch 400/500\n",
      "1176/1176 [==============================] - 1s 511us/step - loss: 0.7512 - acc: 0.8495 - val_loss: 0.8199 - val_acc: 0.8690\n",
      "Epoch 401/500\n",
      "1176/1176 [==============================] - 1s 518us/step - loss: 0.7878 - acc: 0.8401 - val_loss: 0.8469 - val_acc: 0.8651\n",
      "Epoch 402/500\n",
      "1176/1176 [==============================] - 1s 504us/step - loss: 0.7521 - acc: 0.8554 - val_loss: 0.8409 - val_acc: 0.8730\n",
      "Epoch 403/500\n",
      "1176/1176 [==============================] - 1s 495us/step - loss: 0.6707 - acc: 0.8597 - val_loss: 0.8544 - val_acc: 0.8651\n",
      "Epoch 404/500\n",
      "1176/1176 [==============================] - 1s 473us/step - loss: 0.7597 - acc: 0.8529 - val_loss: 0.8614 - val_acc: 0.8671\n",
      "Epoch 405/500\n",
      "1176/1176 [==============================] - 1s 492us/step - loss: 0.7207 - acc: 0.8503 - val_loss: 0.8204 - val_acc: 0.8710\n",
      "Epoch 406/500\n",
      "1176/1176 [==============================] - 1s 484us/step - loss: 0.7579 - acc: 0.8605 - val_loss: 0.8416 - val_acc: 0.8690\n",
      "Epoch 407/500\n",
      "1176/1176 [==============================] - 1s 480us/step - loss: 0.7432 - acc: 0.8427 - val_loss: 0.8390 - val_acc: 0.8690\n",
      "Epoch 408/500\n",
      "1176/1176 [==============================] - 1s 477us/step - loss: 0.7554 - acc: 0.8529 - val_loss: 0.8031 - val_acc: 0.8750\n",
      "Epoch 409/500\n",
      "1176/1176 [==============================] - 1s 473us/step - loss: 0.7529 - acc: 0.8520 - val_loss: 0.7924 - val_acc: 0.8770\n",
      "Epoch 410/500\n",
      "1176/1176 [==============================] - 1s 473us/step - loss: 0.7649 - acc: 0.8571 - val_loss: 0.8000 - val_acc: 0.8750\n",
      "Epoch 411/500\n",
      "1176/1176 [==============================] - 1s 464us/step - loss: 0.6950 - acc: 0.8554 - val_loss: 0.7901 - val_acc: 0.8730\n",
      "Epoch 412/500\n",
      "1176/1176 [==============================] - 1s 467us/step - loss: 0.7593 - acc: 0.8571 - val_loss: 0.7742 - val_acc: 0.8710\n",
      "Epoch 413/500\n",
      "1176/1176 [==============================] - 1s 480us/step - loss: 0.7818 - acc: 0.8486 - val_loss: 0.7909 - val_acc: 0.8770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 414/500\n",
      "1176/1176 [==============================] - 1s 528us/step - loss: 0.7768 - acc: 0.8537 - val_loss: 0.7752 - val_acc: 0.8710\n",
      "Epoch 415/500\n",
      "1176/1176 [==============================] - 1s 567us/step - loss: 0.7327 - acc: 0.8444 - val_loss: 0.8168 - val_acc: 0.8690\n",
      "Epoch 416/500\n",
      "1176/1176 [==============================] - 1s 486us/step - loss: 0.7492 - acc: 0.8571 - val_loss: 0.7881 - val_acc: 0.8750\n",
      "Epoch 417/500\n",
      "1176/1176 [==============================] - 1s 492us/step - loss: 0.7628 - acc: 0.8418 - val_loss: 0.8057 - val_acc: 0.8710\n",
      "Epoch 418/500\n",
      "1176/1176 [==============================] - 1s 499us/step - loss: 0.7834 - acc: 0.8393 - val_loss: 0.8088 - val_acc: 0.8770\n",
      "Epoch 419/500\n",
      "1176/1176 [==============================] - 1s 510us/step - loss: 0.7469 - acc: 0.8656 - val_loss: 0.7878 - val_acc: 0.8770\n",
      "Epoch 420/500\n",
      "1176/1176 [==============================] - 1s 514us/step - loss: 0.8735 - acc: 0.8376 - val_loss: 0.7938 - val_acc: 0.8829\n",
      "Epoch 421/500\n",
      "1176/1176 [==============================] - 1s 484us/step - loss: 0.7080 - acc: 0.8588 - val_loss: 0.7829 - val_acc: 0.8790\n",
      "Epoch 422/500\n",
      "1176/1176 [==============================] - 1s 519us/step - loss: 0.7397 - acc: 0.8478 - val_loss: 0.7728 - val_acc: 0.8770\n",
      "Epoch 423/500\n",
      "1176/1176 [==============================] - 1s 485us/step - loss: 0.7299 - acc: 0.8529 - val_loss: 0.7979 - val_acc: 0.8770\n",
      "Epoch 424/500\n",
      "1176/1176 [==============================] - 1s 482us/step - loss: 0.7534 - acc: 0.8478 - val_loss: 0.7925 - val_acc: 0.8790\n",
      "Epoch 425/500\n",
      "1176/1176 [==============================] - 1s 513us/step - loss: 0.7486 - acc: 0.8614 - val_loss: 0.7703 - val_acc: 0.8770\n",
      "Epoch 426/500\n",
      "1176/1176 [==============================] - 1s 510us/step - loss: 0.7645 - acc: 0.8546 - val_loss: 0.7595 - val_acc: 0.8790\n",
      "Epoch 427/500\n",
      "1176/1176 [==============================] - 1s 497us/step - loss: 0.7910 - acc: 0.8418 - val_loss: 0.7808 - val_acc: 0.8770\n",
      "Epoch 428/500\n",
      "1176/1176 [==============================] - 1s 495us/step - loss: 0.7463 - acc: 0.8554 - val_loss: 0.7637 - val_acc: 0.8750\n",
      "Epoch 429/500\n",
      "1176/1176 [==============================] - 1s 519us/step - loss: 0.7056 - acc: 0.8546 - val_loss: 0.7961 - val_acc: 0.8770\n",
      "Epoch 430/500\n",
      "1176/1176 [==============================] - 1s 566us/step - loss: 0.7817 - acc: 0.8597 - val_loss: 0.7828 - val_acc: 0.8810\n",
      "Epoch 431/500\n",
      "1176/1176 [==============================] - 1s 500us/step - loss: 0.6656 - acc: 0.8690 - val_loss: 0.7495 - val_acc: 0.8810\n",
      "Epoch 432/500\n",
      "1176/1176 [==============================] - 1s 474us/step - loss: 0.7638 - acc: 0.8359 - val_loss: 0.7537 - val_acc: 0.8810\n",
      "Epoch 433/500\n",
      "1176/1176 [==============================] - 1s 467us/step - loss: 0.7497 - acc: 0.8614 - val_loss: 0.7599 - val_acc: 0.8829\n",
      "Epoch 434/500\n",
      "1176/1176 [==============================] - 1s 477us/step - loss: 0.7320 - acc: 0.8478 - val_loss: 0.7329 - val_acc: 0.8849\n",
      "Epoch 435/500\n",
      "1176/1176 [==============================] - 1s 452us/step - loss: 0.6675 - acc: 0.8733 - val_loss: 0.7965 - val_acc: 0.8849\n",
      "Epoch 436/500\n",
      "1176/1176 [==============================] - 1s 468us/step - loss: 0.7628 - acc: 0.8444 - val_loss: 0.7875 - val_acc: 0.8810\n",
      "Epoch 437/500\n",
      "1176/1176 [==============================] - 1s 468us/step - loss: 0.7832 - acc: 0.8478 - val_loss: 0.7606 - val_acc: 0.8790\n",
      "Epoch 438/500\n",
      "1176/1176 [==============================] - 1s 466us/step - loss: 0.6927 - acc: 0.8631 - val_loss: 0.7746 - val_acc: 0.8750\n",
      "Epoch 439/500\n",
      "1176/1176 [==============================] - 1s 464us/step - loss: 0.7580 - acc: 0.8520 - val_loss: 0.7791 - val_acc: 0.8790\n",
      "Epoch 440/500\n",
      "1176/1176 [==============================] - 1s 468us/step - loss: 0.6610 - acc: 0.8631 - val_loss: 0.7713 - val_acc: 0.8790\n",
      "Epoch 441/500\n",
      "1176/1176 [==============================] - 1s 466us/step - loss: 0.7174 - acc: 0.8537 - val_loss: 0.7795 - val_acc: 0.8790\n",
      "Epoch 442/500\n",
      "1176/1176 [==============================] - 1s 466us/step - loss: 0.6885 - acc: 0.8724 - val_loss: 0.8075 - val_acc: 0.8730\n",
      "Epoch 443/500\n",
      "1176/1176 [==============================] - 1s 476us/step - loss: 0.6915 - acc: 0.8597 - val_loss: 0.7783 - val_acc: 0.8829\n",
      "Epoch 444/500\n",
      "1176/1176 [==============================] - 1s 470us/step - loss: 0.7328 - acc: 0.8605 - val_loss: 0.7715 - val_acc: 0.8770\n",
      "Epoch 445/500\n",
      "1176/1176 [==============================] - 1s 475us/step - loss: 0.7241 - acc: 0.8690 - val_loss: 0.7767 - val_acc: 0.8849\n",
      "Epoch 446/500\n",
      "1176/1176 [==============================] - 1s 454us/step - loss: 0.6560 - acc: 0.8673 - val_loss: 0.7786 - val_acc: 0.8790\n",
      "Epoch 447/500\n",
      "1176/1176 [==============================] - 1s 474us/step - loss: 0.6495 - acc: 0.8554 - val_loss: 0.7747 - val_acc: 0.8790\n",
      "Epoch 448/500\n",
      "1176/1176 [==============================] - 1s 470us/step - loss: 0.7268 - acc: 0.8605 - val_loss: 0.7804 - val_acc: 0.8770\n",
      "Epoch 449/500\n",
      "1176/1176 [==============================] - 1s 472us/step - loss: 0.7200 - acc: 0.8563 - val_loss: 0.7815 - val_acc: 0.8790\n",
      "Epoch 450/500\n",
      "1176/1176 [==============================] - 1s 471us/step - loss: 0.6996 - acc: 0.8690 - val_loss: 0.7749 - val_acc: 0.8790\n",
      "Epoch 451/500\n",
      "1176/1176 [==============================] - 1s 469us/step - loss: 0.7032 - acc: 0.8673 - val_loss: 0.7718 - val_acc: 0.8849\n",
      "Epoch 452/500\n",
      "1176/1176 [==============================] - 1s 468us/step - loss: 0.5837 - acc: 0.8673 - val_loss: 0.7466 - val_acc: 0.8889\n",
      "Epoch 453/500\n",
      "1176/1176 [==============================] - 1s 471us/step - loss: 0.6846 - acc: 0.8614 - val_loss: 0.7679 - val_acc: 0.8810\n",
      "Epoch 454/500\n",
      "1176/1176 [==============================] - 1s 473us/step - loss: 0.7454 - acc: 0.8673 - val_loss: 0.7669 - val_acc: 0.8770\n",
      "Epoch 455/500\n",
      "1176/1176 [==============================] - 1s 467us/step - loss: 0.6310 - acc: 0.8690 - val_loss: 0.7686 - val_acc: 0.8810\n",
      "Epoch 456/500\n",
      "1176/1176 [==============================] - 1s 459us/step - loss: 0.7531 - acc: 0.8605 - val_loss: 0.7657 - val_acc: 0.8810\n",
      "Epoch 457/500\n",
      "1176/1176 [==============================] - 1s 478us/step - loss: 0.6573 - acc: 0.8759 - val_loss: 0.7604 - val_acc: 0.8829\n",
      "Epoch 458/500\n",
      "1176/1176 [==============================] - 1s 482us/step - loss: 0.7012 - acc: 0.8614 - val_loss: 0.7800 - val_acc: 0.8790\n",
      "Epoch 459/500\n",
      "1176/1176 [==============================] - 1s 468us/step - loss: 0.6419 - acc: 0.8724 - val_loss: 0.7851 - val_acc: 0.8829\n",
      "Epoch 460/500\n",
      "1176/1176 [==============================] - 1s 486us/step - loss: 0.6765 - acc: 0.8597 - val_loss: 0.7943 - val_acc: 0.8790\n",
      "Epoch 461/500\n",
      "1176/1176 [==============================] - 1s 475us/step - loss: 0.6831 - acc: 0.8639 - val_loss: 0.7738 - val_acc: 0.8810\n",
      "Epoch 462/500\n",
      "1176/1176 [==============================] - 1s 476us/step - loss: 0.6552 - acc: 0.8759 - val_loss: 0.7583 - val_acc: 0.8790\n",
      "Epoch 463/500\n",
      "1176/1176 [==============================] - 1s 464us/step - loss: 0.6945 - acc: 0.8665 - val_loss: 0.7666 - val_acc: 0.8810\n",
      "Epoch 464/500\n",
      "1176/1176 [==============================] - 1s 607us/step - loss: 0.6440 - acc: 0.8784 - val_loss: 0.7491 - val_acc: 0.8810\n",
      "Epoch 465/500\n",
      "1176/1176 [==============================] - 1s 623us/step - loss: 0.6924 - acc: 0.8639 - val_loss: 0.7374 - val_acc: 0.8849\n",
      "Epoch 466/500\n",
      "1176/1176 [==============================] - 1s 621us/step - loss: 0.6723 - acc: 0.8605 - val_loss: 0.7463 - val_acc: 0.8730\n",
      "Epoch 467/500\n",
      "1176/1176 [==============================] - 1s 581us/step - loss: 0.7438 - acc: 0.8529 - val_loss: 0.7531 - val_acc: 0.8790\n",
      "Epoch 468/500\n",
      "1176/1176 [==============================] - 1s 626us/step - loss: 0.6627 - acc: 0.8767 - val_loss: 0.7384 - val_acc: 0.8889\n",
      "Epoch 469/500\n",
      "1176/1176 [==============================] - 1s 495us/step - loss: 0.7202 - acc: 0.8767 - val_loss: 0.7372 - val_acc: 0.8849\n",
      "Epoch 470/500\n",
      "1176/1176 [==============================] - 1s 471us/step - loss: 0.6469 - acc: 0.8648 - val_loss: 0.7424 - val_acc: 0.8849\n",
      "Epoch 471/500\n",
      "1176/1176 [==============================] - 1s 478us/step - loss: 0.6388 - acc: 0.8665 - val_loss: 0.7495 - val_acc: 0.8869\n",
      "Epoch 472/500\n",
      "1176/1176 [==============================] - 1s 474us/step - loss: 0.6532 - acc: 0.8605 - val_loss: 0.7353 - val_acc: 0.8869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 473/500\n",
      "1176/1176 [==============================] - 1s 457us/step - loss: 0.6354 - acc: 0.8801 - val_loss: 0.7699 - val_acc: 0.8849\n",
      "Epoch 474/500\n",
      "1176/1176 [==============================] - 1s 443us/step - loss: 0.6929 - acc: 0.8648 - val_loss: 0.7822 - val_acc: 0.8750\n",
      "Epoch 475/500\n",
      "1176/1176 [==============================] - 1s 442us/step - loss: 0.6684 - acc: 0.8724 - val_loss: 0.7848 - val_acc: 0.8750\n",
      "Epoch 476/500\n",
      "1176/1176 [==============================] - 1s 472us/step - loss: 0.6340 - acc: 0.8665 - val_loss: 0.7839 - val_acc: 0.8730\n",
      "Epoch 477/500\n",
      "1176/1176 [==============================] - 1s 484us/step - loss: 0.6224 - acc: 0.8937 - val_loss: 0.7780 - val_acc: 0.8770\n",
      "Epoch 478/500\n",
      "1176/1176 [==============================] - 1s 524us/step - loss: 0.5931 - acc: 0.8801 - val_loss: 0.7840 - val_acc: 0.8790\n",
      "Epoch 479/500\n",
      "1176/1176 [==============================] - 1s 634us/step - loss: 0.6739 - acc: 0.8707 - val_loss: 0.7462 - val_acc: 0.8750\n",
      "Epoch 480/500\n",
      "1176/1176 [==============================] - 1s 447us/step - loss: 0.7211 - acc: 0.8648 - val_loss: 0.7988 - val_acc: 0.8770\n",
      "Epoch 481/500\n",
      "1176/1176 [==============================] - 1s 430us/step - loss: 0.6235 - acc: 0.8827 - val_loss: 0.7961 - val_acc: 0.8790\n",
      "Epoch 482/500\n",
      "1176/1176 [==============================] - 0s 424us/step - loss: 0.6473 - acc: 0.8614 - val_loss: 0.7568 - val_acc: 0.8829\n",
      "Epoch 483/500\n",
      "1176/1176 [==============================] - 1s 431us/step - loss: 0.6441 - acc: 0.8793 - val_loss: 0.7912 - val_acc: 0.8750\n",
      "Epoch 484/500\n",
      "1176/1176 [==============================] - 0s 423us/step - loss: 0.7092 - acc: 0.8588 - val_loss: 0.7531 - val_acc: 0.8770\n",
      "Epoch 485/500\n",
      "1176/1176 [==============================] - 1s 437us/step - loss: 0.6429 - acc: 0.8784 - val_loss: 0.7873 - val_acc: 0.8810\n",
      "Epoch 486/500\n",
      "1176/1176 [==============================] - 1s 439us/step - loss: 0.6377 - acc: 0.8733 - val_loss: 0.7690 - val_acc: 0.8730\n",
      "Epoch 487/500\n",
      "1176/1176 [==============================] - 1s 431us/step - loss: 0.7294 - acc: 0.8648 - val_loss: 0.7560 - val_acc: 0.8770\n",
      "Epoch 488/500\n",
      "1176/1176 [==============================] - 1s 431us/step - loss: 0.6956 - acc: 0.8716 - val_loss: 0.7780 - val_acc: 0.8810\n",
      "Epoch 489/500\n",
      "1176/1176 [==============================] - 1s 435us/step - loss: 0.6248 - acc: 0.8707 - val_loss: 0.7562 - val_acc: 0.8770\n",
      "Epoch 490/500\n",
      "1176/1176 [==============================] - 1s 431us/step - loss: 0.7028 - acc: 0.8716 - val_loss: 0.7679 - val_acc: 0.8810\n",
      "Epoch 491/500\n",
      "1176/1176 [==============================] - 1s 427us/step - loss: 0.6393 - acc: 0.8801 - val_loss: 0.7475 - val_acc: 0.8829\n",
      "Epoch 492/500\n",
      "1176/1176 [==============================] - 1s 431us/step - loss: 0.5839 - acc: 0.8767 - val_loss: 0.7534 - val_acc: 0.8790\n",
      "Epoch 493/500\n",
      "1176/1176 [==============================] - 0s 422us/step - loss: 0.7131 - acc: 0.8631 - val_loss: 0.7577 - val_acc: 0.8790\n",
      "Epoch 494/500\n",
      "1176/1176 [==============================] - 0s 424us/step - loss: 0.6869 - acc: 0.8690 - val_loss: 0.7689 - val_acc: 0.8770\n",
      "Epoch 495/500\n",
      "1176/1176 [==============================] - 0s 425us/step - loss: 0.6231 - acc: 0.8707 - val_loss: 0.7445 - val_acc: 0.8810\n",
      "Epoch 496/500\n",
      "1176/1176 [==============================] - 1s 428us/step - loss: 0.6575 - acc: 0.8784 - val_loss: 0.7507 - val_acc: 0.8810\n",
      "Epoch 497/500\n",
      "1176/1176 [==============================] - 1s 428us/step - loss: 0.6121 - acc: 0.8733 - val_loss: 0.7587 - val_acc: 0.8790\n",
      "Epoch 498/500\n",
      "1176/1176 [==============================] - 1s 426us/step - loss: 0.6298 - acc: 0.8724 - val_loss: 0.7538 - val_acc: 0.8810\n",
      "Epoch 499/500\n",
      "1176/1176 [==============================] - 1s 428us/step - loss: 0.6816 - acc: 0.8733 - val_loss: 0.7579 - val_acc: 0.8810\n",
      "Epoch 500/500\n",
      "1176/1176 [==============================] - 0s 425us/step - loss: 0.6438 - acc: 0.8793 - val_loss: 0.7596 - val_acc: 0.8829\n"
     ]
    }
   ],
   "source": [
    "train_result = model.fit(np.array(X_train), y_train_norm,\n",
    "          batch_size=8,\n",
    "          epochs=500,\n",
    "          verbose=1,\n",
    "          shuffle = True,\n",
    "         validation_data=(np.array(X_test), y_test_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('lpc_model_weights_' + str(audio_len) + '_' + str(window_size) + '-' + str(ctr) + '.h5')\n",
    "ctr += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "plt.plot(train_result.history['acc'], color=\"#5599FF\")\n",
    "plt.plot(train_result.history['val_acc'], color=\"#55FF99\")\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
